{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/alexpod1000/SQuAD-QA/blob/main/ModelTrainExperimentalCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oTkVfFrJ-pzG"
   },
   "outputs": [],
   "source": [
    "#%%bash\n",
    "#[[ ! -e /colabtools ]] && exit  # Continue only if running on Google Colab\n",
    "\n",
    "# Clone repository\n",
    "# https://sysadmins.co.za/clone-a-private-github-repo-with-personal-access-token/\n",
    "# For cloning the main branch:\n",
    "#!git clone https://fb5b65b126107273e595ce8b6c9d2d533103c6e2:x-oauth-basic@github.com/alexpod1000/SQuAD-QA.git\n",
    "# For cloning the \"evaluation-features\" branch\n",
    "#!git clone --branch evaluation-features https://fb5b65b126107273e595ce8b6c9d2d533103c6e2:x-oauth-basic@github.com/alexpod1000/SQuAD-QA.git\n",
    "# Change current working directory to match project\n",
    "#%cd SQuAD-QA/\n",
    "#!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rsBVuJu6_5qN"
   },
   "outputs": [],
   "source": [
    "# External imports\n",
    "import copy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import torch\n",
    "\n",
    "from functools import partial\n",
    "from nltk.tokenize import TreebankWordTokenizer, SpaceTokenizer\n",
    "from typing import Tuple, List, Dict, Any, Union\n",
    "\n",
    "# Project imports\n",
    "from squad_data.parser import SquadFileParser\n",
    "from squad_data.utils import build_mappers_and_dataframe, add_paragraphs_spans\n",
    "from evaluation.evaluation_metrics import Evaluator\n",
    "from evaluation.utils import extract_answer, build_evaluation_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "387a021D9piE"
   },
   "source": [
    "### Download Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xFweP2uIJg8O",
    "outputId": "7bb5b9ca-7b89-4fad-dc90-c979f6ef5f0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-downloaded embeddings from /home/alexpod/uni/magistrale_ai/secondo_anno/nlp/project/SQuAD-QA/embedding_models/embedding_model.kv\n",
      "End!\n",
      "Embedding dimension: 300\n"
     ]
    }
   ],
   "source": [
    "from utils.embedding_utils import EmbeddingDownloader\n",
    "\n",
    "embedding_downloader = EmbeddingDownloader(\n",
    "    \"embedding_models\", \n",
    "    \"embedding_model.kv\", \n",
    "    model_name=\"fasttext-wiki-news-subwords-300\"\n",
    ")\n",
    "\n",
    "embedding_model = embedding_downloader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Rh4dSW-9tYm"
   },
   "source": [
    "### Parse the json and get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FAEEYoypAOKA"
   },
   "outputs": [],
   "source": [
    "parser = SquadFileParser(\"squad_data/data/training_set.json\")\n",
    "data = parser.parse_documents()\n",
    "\n",
    "########################### DEBUG\n",
    "# reduce size for faster testing\n",
    "#full_data = data\n",
    "#data = []\n",
    "#for i in range(1): # use only the first 1 documents\n",
    "#  data.append(full_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKK-4d1_93QE"
   },
   "source": [
    "### Prepare the mappers and datafram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenizer_fn(question, paragraph, tokenizer, max_length=384):\n",
    "    doc_stride = 128\n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "    # TODO: add max_length and doc_stride\n",
    "    tokenized_input_pair = tokenizer(\n",
    "        question,\n",
    "        paragraph,\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    return tokenized_input_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "from typing import Union, Tuple, List, Dict, Any\n",
    "from squad_data import Document\n",
    "\n",
    "def index_of_first(lst, pred):\n",
    "    for i, v in enumerate(lst):\n",
    "        if pred(v):\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def split_paragraph_if_needed(paragraph, question, answer_span, tokenizer_fn):\n",
    "    \"\"\"\n",
    "    Attempts to tokenize a paragraph and question together, if too long\n",
    "    because of tokenizer's max length, then will split the paragraph into\n",
    "    multiple slices.\n",
    "    \n",
    "    Returns a list of paragraph slices with answer span, such that:\n",
    "        - a paragraph slice with no answer will have answer mapped to (CLS, CLS)\n",
    "        - a paragraph slice with answer will be mapped to the index of answer.\n",
    "    \"\"\"\n",
    "    tokenized_input_pair = tokenizer_fn(question, paragraph)\n",
    "    # outputs\n",
    "    paragraph_splits = []\n",
    "    answer_spans = []\n",
    "    # get answer end char idx\n",
    "    ans_start = answer_span[0]\n",
    "    ans_end = answer_span[1]\n",
    "    \n",
    "    #n_special_tokens = 3 # there are at least 3 special tokens introduced by tokenizer\n",
    "    \"\"\"\n",
    "    1) Find index of context segments in the tokenized example\n",
    "    2) Within the context segments (start from context_segment_idx), \n",
    "       find the token corresponding to span of answer: start and end.\n",
    "    \"\"\"\n",
    "    for offset_idx, offset in enumerate(tokenized_input_pair.offset_mapping):\n",
    "        # get sequence ids\n",
    "        sequence_ids = tokenized_input_pair.sequence_ids(offset_idx)\n",
    "        # find start index of context segment\n",
    "        context_segment_idx = sequence_ids.index(1)\n",
    "        # TODO(Alex): ADD QUICK FIX WITH n_special_tokens (but it's not a proper solution)\n",
    "        span_start_offset_idx = index_of_first(\n",
    "            tokenized_input_pair.offset_mapping[offset_idx][context_segment_idx:], \n",
    "            lambda span: span[0] <= ans_start <= span[1]\n",
    "        )\n",
    "        span_end_offset_idx = index_of_first(\n",
    "            tokenized_input_pair.offset_mapping[offset_idx][context_segment_idx:], \n",
    "            lambda span: span[0] <= ans_end <= span[1]\n",
    "        )\n",
    "        # Decode split into a string\n",
    "        decoded_split = tokenizer.decode(tokenized_input_pair.input_ids[offset_idx][context_segment_idx:], skip_special_tokens=True)\n",
    "        # \n",
    "        paragraph_splits.append(decoded_split)\n",
    "        if span_start_offset_idx is not None and span_end_offset_idx is not None:\n",
    "            # If answer span is fully in current slice\n",
    "            # add segment idx offset\n",
    "            span_start_offset_idx += context_segment_idx\n",
    "            span_end_offset_idx += context_segment_idx + 1 # the plus 1 is needed for correct slicing\n",
    "            answer_spans.append((span_start_offset_idx, span_end_offset_idx))\n",
    "            # TODO: we have span indexes now, map them to groundtruth (?)\n",
    "        elif span_start_offset_idx is None and span_end_offset_idx is None:\n",
    "            # If span not in this slice, but in another slice\n",
    "            # map answer to (CLS, CLS)\n",
    "            cls_idx = tokenized_input_pair.input_ids[offset_idx].index(tokenizer.cls_token_id)\n",
    "            # NOTE(Alex): although I think it's always 0\n",
    "            answer_spans.append((cls_idx, cls_idx))\n",
    "        else:\n",
    "            # span spans along multiple slices -> throw the sample away \n",
    "            # (should be only like 4 samples across the whole dataset)\n",
    "            # Discard sample\n",
    "            pass\n",
    "    \n",
    "    return (paragraph_splits, answer_spans)\n",
    "\n",
    "def build_bert_mappers_and_dataframe(\n",
    "    tokenizer_fn,\n",
    "    documents_list: List[Document], \n",
    "    limit_answers: int = -1\n",
    "    ) -> Tuple[Dict[str, str], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Given a list of SQuAD Document objects, returns mapper to transform from\n",
    "    paragraph id to paragraph text and a dataframe containing paragraph id, \n",
    "    question id, text and answer details.\n",
    "    Args:\n",
    "        tokenizer_fn: Huggingface tokenizer\n",
    "        documents_list (List[Document]): list of parsed SQuAD document objects.\n",
    "        limit_answers (int): limit number of returned answers per question\n",
    "            to this amount (-1 to return all the available answers).\n",
    "\n",
    "    Returns:\n",
    "        paragraphs_mapper: mapper from paragraph id to paragraph text\n",
    "        dataframe: Pandas dataframe with the following schema\n",
    "            (paragraph_id, question_id, question_text, answer_id, answer_start, answer_text)\n",
    "    \"\"\"\n",
    "\n",
    "    # type for np array: np.ndarray\n",
    "    # given a paragraph id, maps the paragraph to its text or embeddings (or both)\n",
    "    split_paragraphs_mapper = {}\n",
    "    # dataframe\n",
    "    dataframe_list = []\n",
    "    for doc_idx, document in enumerate(documents_list):\n",
    "        # for each paragraph\n",
    "        for par_idx, paragraph in enumerate(document.paragraphs):\n",
    "            par_text = paragraph.context.strip()\n",
    "\n",
    "            # for each question\n",
    "            for question in paragraph.questions:\n",
    "                question_id = question.id\n",
    "                question_text = question.question.strip()\n",
    "\n",
    "                # TODO(Alex): at this level, could need to start building par/question\n",
    "\n",
    "                # take only \"limit_answers\" answers for every question.\n",
    "                answer_range = len(question.answers) if limit_answers == -1 else limit_answers\n",
    "                for answer_id, answer in enumerate(question.answers[:answer_range]):\n",
    "                    # NOTE: in training set, there's only one answer per question.\n",
    "                    answer_text = answer.text.strip()\n",
    "                    # get span\n",
    "                    answer_start = answer.answer_start\n",
    "                    answer_end = answer.answer_start + len(answer_text)\n",
    "\n",
    "                    par_splits, split_answer_spans = split_paragraph_if_needed(par_text, question_text, (answer_start, answer_end), tokenizer_fn)\n",
    "                    \n",
    "                    pair_overflows = len(par_splits) > 1\n",
    "                    \n",
    "                    for split_idx, (split_text, split_ans_span) in enumerate(zip(par_splits, split_answer_spans)):\n",
    "                        \"\"\"\n",
    "                        NOTE(Alex): since in tokenization phase we also use question, our ID depends on question too\n",
    "                                    For example if for question1, the pair <question1, par> goes above the limit,\n",
    "                                    but for <question2, par> it does not, then we'll still need to keep track of\n",
    "                                    different splits of par, depending on each question.\n",
    "                        \"\"\"\n",
    "                        if pair_overflows:\n",
    "                            split_par_id = \"{}_{}_{}_{}\".format(doc_idx, par_idx, question_id, split_idx)\n",
    "                        else:\n",
    "                            \"\"\"\n",
    "                            If no length overflow, then we don't need question_id or split_idx\n",
    "                            To optimize memory, we can map same splits to same id \n",
    "                            (some pairs <question, par> won't overflow anyway)\n",
    "                            \"\"\"\n",
    "                            split_par_id = \"{}_{}\".format(doc_idx, par_idx)\n",
    "                        split_paragraphs_mapper[split_par_id] = split_text\n",
    "                        # build dataframe entry\n",
    "                        dataframe_list.append({\n",
    "                            \"paragraph_id\": split_par_id,\n",
    "                            \"question_id\": question_id,\n",
    "                            \"answer_id\": answer_id,\n",
    "                            \"answer_start\": answer_start,\n",
    "                            \"answer_text\": answer_text,\n",
    "                            \"question_text\": question_text,\n",
    "                            \"tokenizer_answer_start\": split_ans_span[0],\n",
    "                            \"tokenizer_answer_end\": split_ans_span[1],\n",
    "                        })\n",
    "    return split_paragraphs_mapper, pd.DataFrame(dataframe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer_fn_preprocess = partial(bert_tokenizer_fn, tokenizer=tokenizer, max_length=380)\n",
    "tokenizer_fn_train = partial(bert_tokenizer_fn, tokenizer=tokenizer, max_length=384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#par_text, ques_text, ans_start, ans_text = build_bert_mappers_and_dataframe(tokenizer, data, limit_answers=1)\n",
    "paragraphs_mapper, df = build_bert_mappers_and_dataframe(tokenizer_fn_preprocess, data, limit_answers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Custom_BERT_QADataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom text dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer_fn, df, paragraphs_mapper):\n",
    "        self.input_list = df[[\"paragraph_id\", \"question_text\", \"question_id\"]]\n",
    "        self.output_list = df[[\"tokenizer_answer_start\", \"tokenizer_answer_end\"]]\n",
    "        self.paragraphs_mapper = paragraphs_mapper\n",
    "        self.tokenizer_fn = tokenizer_fn\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        paragraph_id = self.input_list.iloc[idx][\"paragraph_id\"]\n",
    "        question_id = self.input_list.iloc[idx][\"question_id\"]\n",
    "        question_text = self.input_list.iloc[idx][\"question_text\"]\n",
    "        tokenizer_answer_start = self.output_list.iloc[idx][\"tokenizer_answer_start\"]\n",
    "        tokenizer_answer_end = self.output_list.iloc[idx][\"tokenizer_answer_end\"]\n",
    "\n",
    "        paragraph_text = self.paragraphs_mapper[paragraph_id]\n",
    "        tokenized_input_pair = self.tokenizer_fn(question_text, paragraph_text)\n",
    "        \n",
    "        #input_ids = torch.tensor(tokenized_input_pair[\"input_ids\"], dtype=torch.long)\n",
    "        #attention_mask = torch.tensor(tokenized_input_pair[\"attention_mask\"], dtype=torch.long)\n",
    "        input_ids = tokenized_input_pair[\"input_ids\"]\n",
    "        attention_mask = tokenized_input_pair[\"attention_mask\"]\n",
    "\n",
    "        out_span = torch.tensor([tokenizer_answer_start, tokenizer_answer_end])\n",
    "        \n",
    "        # DistilBERT doesnâ€™t have token_type_ids\n",
    "        \n",
    "        return input_ids, attention_mask, out_span, paragraph_id, question_id, idx\n",
    "    \n",
    "def bert_padder_collate_fn(sample_list):\n",
    "    # NOTE: the tokenizer in dataloader already pads inputs to have same length of 384\n",
    "    input_ids_padded = [sample[0] for sample in sample_list]\n",
    "    attention_mask_padded = [sample[1] for sample in sample_list]\n",
    "    out = [sample[2] for sample in sample_list]\n",
    "    paragraph_id = [sample[3] for sample in sample_list]\n",
    "    question_id = [sample[4] for sample in sample_list]\n",
    "    debug_idx = [sample[5] for sample in sample_list]\n",
    "    \n",
    "    #input_ids_padded = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True)\n",
    "    #attention_mask_padded = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True)\n",
    "    \n",
    "    prev_shape_inp = None\n",
    "    prev_shape_attn = None\n",
    "    \n",
    "    try:\n",
    "        input_ids_padded = torch.tensor(input_ids_padded, dtype=torch.long)\n",
    "        attention_mask_padded = torch.tensor(attention_mask_padded, dtype=torch.long)\n",
    "        \n",
    "        prev_shape_inp = input_ids_padded.shape\n",
    "        prev_shape_attn = attention_mask_padded.shape\n",
    "        \n",
    "        input_ids_padded = input_ids_padded[:, 0, :]\n",
    "        attention_mask_padded = attention_mask_padded[:, 0, :]\n",
    "    except:\n",
    "        print(paragraph_id)\n",
    "        print(question_id)\n",
    "        print(debug_idx)\n",
    "        print(prev_shape_inp)\n",
    "        print(prev_shape_attn)\n",
    "    \n",
    "    #answer_emb_padded = torch.nn.utils.rnn.pad_sequence(out, batch_first=True)\n",
    "    return {\"input_ids\": input_ids_padded,\n",
    "            \"attention_mask\": attention_mask_padded,\n",
    "            \"y_gt\":torch.stack(out),\n",
    "            \"paragraph_id\":paragraph_id,\n",
    "            \"question_id\":question_id}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertBaseQA(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, num_labels):\n",
    "        super(DistilBertBaseQA, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        DistilBertConfig Model Test\n",
    "        \"\"\"\n",
    "        bert_config = transformers.DistilBertConfig(max_position_embeddings=384)\n",
    "        bert_model = transformers.DistilBertModel(bert_config)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_labels = num_labels\n",
    "        self.config = transformers.DistilBertConfig(max_position_embeddings=384)\n",
    "        self.bert = transformers.DistilBertModel(bert_config)\n",
    "        self.qa_outputs = torch.nn.Linear(self.hidden_size, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
    "\n",
    "        sequence_output = output[0]   #(None, seq_len, hidden_size)\n",
    "        logits = self.qa_outputs(sequence_output) #(None, seq_len, hidden_size)*(hidden_size, 2)=(None, seq_len, 2)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)    #(None, seq_len, 1), (None, seq_len, 1)\n",
    "        start_logits = start_logits.squeeze(-1)  #(None, seq_len)\n",
    "        end_logits = end_logits.squeeze(-1)    #(None, seq_len)\n",
    "\n",
    "\n",
    "        outputs = (start_logits, end_logits,) \n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetBertQA = Custom_BERT_QADataset(tokenizer_fn_train, df, paragraphs_mapper)\n",
    "data_loader = torch.utils.data.DataLoader(datasetBertQA, collate_fn = bert_padder_collate_fn, batch_size=10, shuffle=True)\n",
    "\n",
    "test_batch = next(iter(data_loader))\n",
    "#print(test_batch[\"paragraph_emb\"].shape)\n",
    "#print(test_batch[\"y_gt\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertBaseQA(768, 2).to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(datasetBertQA, collate_fn = bert_padder_collate_fn, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = [23768, 22675, 44840, 70921, 17668, 83128, 2196, 24744, 76377, 20475, 61752, 74694, 84931, 77013, 8950, 88488]\n",
    "failed = bert_padder_collate_fn([datasetBertQA[i] for i in idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failed it failed precisely on: 418_19_57312110a5e9cc1400cdbc57_0\n",
    "\n",
    "['102_68', '99_18', '210_62', '359_5', '76_48', '418_19_57312110a5e9cc1400cdbc57_0', '5_74', '107_9', '383_22', '89_25', '304_55', '374_28', '424_63', '388_5', '22_24', '441_55']\n",
    "['56fb85aab28b3419009f1dfc', '56fa74058f12f31900630149', '5727ea07ff5b5019007d9864', '572e83d3c246551400ce429f', '56e78a2e00c9c71400d77270', '57312110a5e9cc1400cdbc57', '56d3a2cd59d6e4140014684a', '5705e91d52bb891400689681', '572f51f5a23a5019007fc538', '56f7f909a6d7ea1400e17343', '5728a4d44b864d1900164b4e', '57307b79069b531400832116', '573212f70fdd8d15006c6760', '572f6db904bcaa1900d76935', '5733229dd058e614000b571a', '5735d0f46c16ec1900b92824']\n",
    "[23768, 22675, 44840, 70921, 17668, 83128, 2196, 24744, 76377, 20475, 61752, 74694, 84931, 77013, 8950, 88488]\n",
    "None\n",
    "None\n",
    "\n",
    "\n",
    "Might be because we use original, not tokenized question together with a detokenized context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Like the reptiles, birds are primarily uricotelic, that is, their kidneys extract nitrogenous waste from their bloodstream and excrete it as uric acid instead of urea or ammonia through the ureters into the intestine. Birds do not have a urinary bladder or external urethral opening and (with exception of the ostrich) uric acid is excreted along with feces as a semisolid waste. However, birds such as hummingbirds can be facultatively ammonotelic, excreting most of the nitrogenous wastes as ammonia. They also excrete creatine, rather than creatinine like mammals. This material, as well as the output of the intestines, emerges from the bird's cloaca. The cloaca is a multi-purpose opening: waste is expelled through it, most birds mate by joining cloaca, and females lay eggs from it. In addition, many species of birds regurgitate pellets. Males within Palaeognathae (with the exception of the kiwis), the Anseriformes (with the exception of screamers), and in rudimentary forms in Galliformes (but fully developed in Cracidae) possess a penis, which is never present in Neoaves. The length is thought to be related to sperm competition. When not copulating, it is hidden within the proctodeum compartment within the cloaca, just inside the vent. The digestive system of birds is unique, with a crop for storage and a gizzard that contains swallowed stones for grinding food to compensate for the lack of teeth. Most birds are highly adapted for rapid digestion to aid with flight. Some migratory birds have adapted to use protein from many parts of their bodies, including protein from the intestines, as additional energy during migration.\n",
      "What aids birds with flight?\n"
     ]
    }
   ],
   "source": [
    "print(data[418].paragraphs[19].context)\n",
    "print(data[418].paragraphs[19].questions[4].question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "thefuck = tokenizer_fn(data[418].paragraphs[19].questions[4].question, data[418].paragraphs[19].context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq = tokenizer.decode(thefuck.input_ids[0][:thefuck.sequence_ids(0).index(1)], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what aids birds with flight?'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "omg = tokenizer.decode(thefuck.input_ids[0])\n",
    "#omg = tokenizer.decode(thefuck.input_ids[0][thefuck.sequence_ids(0).index(1):-2], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] what aids birds with flight? [SEP] like the reptiles, birds are primarily uricotelic, that is, their kidneys extract nitrogenous waste from their bloodstream and excrete it as uric acid instead of urea or ammonia through the ureters into the intestine. birds don't have a urinary bladder or external urethral opening and ( with exception of the ostrich ) uric acid is excreted along with feces as a semisolid waste. however, birds such as hummingbirds can be facultatively ammonotelic, excreting most of the nitrogenous wastes as ammonia. they also excrete creatine, rather than creatinine like mammals. this material, as well as the output of the intestines, emerges from the bird's cloaca. the cloaca is a multi - purpose opening : waste is expelled through it, most birds mate by joining cloaca, and females lay eggs from it. in addition, many species of birds regurgitate pellets. males within palaeognathae ( with the exception of the kiwis ), the anseriformes ( with the exception of screamers ), and in rudimentary forms in galliformes ( but fully developed in cracidae ) possess a penis, which is never present in neoaves. the length is thought to be related to sperm competition. when not copulating, it is hidden within the proctodeum compartment within the cloaca, just inside the vent. the digestive system of birds is unique, with a crop for storage and a gizzard that contains swallowed stones for grinding food to compensate for the lack of teeth. most birds are highly adapted for rapid digestion to aid with flight. some migratory birds have adapted to use protein from many parts of [SEP]\""
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "thefuck2 = tokenizer_fn(data[418].paragraphs[19].questions[4].question, omg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(thefuck2.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "thefuck3 = tokenizer_fn('what aids birds with flight?', omg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(thefuck3.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and in rudimentary forms in galliformes ( but fully developed in cracidae ) possess a penis, which is never present in neoaves. the length is thought to be related to sperm competition. when not copulating, it is hidden within the proctodeum compartment within the cloaca, just inside the vent. the digestive system of birds is unique, with a crop for storage and a gizzard that contains swallowed stones for grinding food to compensate for the lack of teeth. most birds are highly adapted for rapid digestion to aid with flight. some migratory birds have adapted to use protein from many parts of their bodies, including protein from the intestines, as additional energy during migration.'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs_mapper[\"418_19_57312110a5e9cc1400cdbc57_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] what aids birds with flight? [SEP] like the reptiles, birds are primarily uricotelic, that is, their kidneys extract nitrogenous waste from their bloodstream and excrete it as uric acid instead of urea or ammonia through the ureters into the intestine. birds don't have a urinary bladder or external urethral opening and ( with exception of the ostrich ) uric acid is excreted along with feces as a semisolid waste. however, birds such as hummingbirds can be facultatively ammonotelic, excreting most of the nitrogenous wastes as ammonia. they also excrete creatine, rather than creatinine like mammals. this material, as well as the output of the intestines, emerges from the bird's cloaca. the cloaca is a multi - purpose opening : waste is expelled through it, most birds mate by joining cloaca, and females lay eggs from it. in addition, many species of birds regurgitate pellets. males within palaeognathae ( with the exception of the kiwis ), the anseriformes ( with the exception of screamers ), and in rudimentary forms in galliformes ( but fully developed in cracidae ) possess a penis, which is never present in neoaves. the length is thought to be related to sperm competition. when not copulating, it is hidden within the proctodeum compartment within the cloaca, just inside the vent. the digestive system of birds is unique, with a crop for storage and a gizzard that contains swallowed stones for grinding food to compensate for the lack of teeth. most birds are highly adapted for rapid digestion to aid with flight. some migratory birds have adapted to use protein from many parts [SEP]\n"
     ]
    }
   ],
   "source": [
    "offset_idx = 0\n",
    "sequence_ids = tttt.sequence_ids(offset_idx)\n",
    "# find start index of context segment\n",
    "context_segment_idx = sequence_ids.index(1)\n",
    "# Decode split into a string\n",
    "decoded_split = tokenizer.decode(tttt.input_ids[offset_idx])\n",
    "print(decoded_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n"
     ]
    }
   ],
   "source": [
    "print(len(decoded_split.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>question_text</th>\n",
       "      <th>tokenizer_answer_start</th>\n",
       "      <th>tokenizer_answer_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83178</th>\n",
       "      <td>418_19_57312110a5e9cc1400cdbc57_0</td>\n",
       "      <td>57312110a5e9cc1400cdbc57</td>\n",
       "      <td>0</td>\n",
       "      <td>1452</td>\n",
       "      <td>rapid digestion</td>\n",
       "      <td>What aids birds with flight?</td>\n",
       "      <td>363</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            paragraph_id               question_id  answer_id  \\\n",
       "83178  418_19_57312110a5e9cc1400cdbc57_0  57312110a5e9cc1400cdbc57          0   \n",
       "\n",
       "       answer_start      answer_text                 question_text  \\\n",
       "83178          1452  rapid digestion  What aids birds with flight?   \n",
       "\n",
       "       tokenizer_answer_start  tokenizer_answer_end  \n",
       "83178                     363                   366  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.paragraph_id == \"418_19_57312110a5e9cc1400cdbc57_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>question_text</th>\n",
       "      <th>tokenizer_answer_start</th>\n",
       "      <th>tokenizer_answer_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83129</th>\n",
       "      <td>418_19_57312110a5e9cc1400cdbc57_1</td>\n",
       "      <td>57312110a5e9cc1400cdbc57</td>\n",
       "      <td>0</td>\n",
       "      <td>1452</td>\n",
       "      <td>rapid digestion</td>\n",
       "      <td>What aids birds with flight?</td>\n",
       "      <td>116</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            paragraph_id               question_id  answer_id  \\\n",
       "83129  418_19_57312110a5e9cc1400cdbc57_1  57312110a5e9cc1400cdbc57          0   \n",
       "\n",
       "       answer_start      answer_text                 question_text  \\\n",
       "83129          1452  rapid digestion  What aids birds with flight?   \n",
       "\n",
       "       tokenizer_answer_start  tokenizer_answer_end  \n",
       "83129                     116                   119  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.paragraph_id == \"418_19_57312110a5e9cc1400cdbc57_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tttt = tokenizer_fn_train(\"What aids birds with flight?\", paragraphs_mapper[\"418_19_57312110a5e9cc1400cdbc57_0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs_mod = model(torch.tensor(tttt[\"input_ids\"]).to(device), torch.tensor(tttt[\"attention_mask\"]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2]), tensor([184]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SpanExtractor.extract_most_probable(outs_mod[0], outs_mod[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 384])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs_mod[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2054, 8387, 5055, 2007, 3462, 1029, 102, 2066, 1996, 20978, 1010, 5055, 2024, 3952, 24471, 11261, 9834, 2594, 1010, 2008, 2003, 1010, 2037, 14234, 2015, 14817, 14114, 3560, 5949, 2013, 2037, 2668, 21422, 1998, 4654, 16748, 2618, 2009, 2004, 24471, 2594, 5648, 2612, 1997, 24471, 5243, 2030, 25874, 2083, 1996, 24471, 15141, 2015, 2046, 1996, 20014, 4355, 3170, 1012, 5055, 2123, 1005, 1056, 2031, 1037, 24471, 3981, 2854, 24176, 2030, 6327, 24471, 11031, 7941, 3098, 1998, 1006, 2007, 6453, 1997, 1996, 9808, 12412, 2232, 1007, 24471, 2594, 5648, 2003, 4654, 16748, 3064, 2247, 2007, 10768, 9623, 2004, 1037, 4100, 19454, 3593, 5949, 1012, 2174, 1010, 5055, 2107, 2004, 20364, 12887, 2064, 2022, 6904, 10841, 24458, 25499, 2572, 8202, 12184, 10415, 1010, 4654, 16748, 3436, 2087, 1997, 1996, 14114, 3560, 5949, 2015, 2004, 25874, 1012, 2027, 2036, 4654, 16748, 2618, 13675, 5243, 10196, 1010, 2738, 2084, 13675, 5243, 7629, 3170, 2066, 11993, 1012, 2023, 3430, 1010, 2004, 2092, 2004, 1996, 6434, 1997, 1996, 20014, 4355, 10586, 1010, 19391, 2013, 1996, 4743, 1005, 1055, 18856, 10441, 3540, 1012, 1996, 18856, 10441, 3540, 2003, 1037, 4800, 1011, 3800, 3098, 1024, 5949, 2003, 10016, 2083, 2009, 1010, 2087, 5055, 6775, 2011, 5241, 18856, 10441, 3540, 1010, 1998, 3801, 3913, 6763, 2013, 2009, 1012, 1999, 2804, 1010, 2116, 2427, 1997, 5055, 19723, 12514, 17570, 21877, 22592, 2015, 1012, 3767, 2306, 14412, 6679, 8649, 16207, 6679, 1006, 2007, 1996, 6453, 1997, 1996, 11382, 9148, 2015, 1007, 1010, 1996, 2019, 8043, 22631, 2229, 1006, 2007, 1996, 6453, 1997, 6978, 2545, 1007, 1010, 1998, 1999, 21766, 21341, 5649, 3596, 1999, 26033, 22631, 2229, 1006, 2021, 3929, 2764, 1999, 13675, 6305, 6096, 1007, 10295, 1037, 19085, 1010, 2029, 2003, 2196, 2556, 1999, 9253, 21055, 1012, 1996, 3091, 2003, 2245, 2000, 2022, 3141, 2000, 18047, 2971, 1012, 2043, 2025, 8872, 10924, 1010, 2009, 2003, 5023, 2306, 1996, 4013, 6593, 10244, 2819, 15273, 2306, 1996, 18856, 10441, 3540, 1010, 2074, 2503, 1996, 18834, 1012, 1996, 17886, 3512, 2291, 1997, 5055, 2003, 4310, 1010, 2007, 1037, 10416, 2005, 5527, 1998, 1037, 21025, 20715, 4103, 2008, 3397, 7351, 6386, 2005, 16153, 2833, 2000, 19079, 2005, 1996, 3768, 1997, 4091, 1012, 2087, 5055, 2024, 3811, 5967, 2005, 5915, 17886, 3258, 2000, 4681, 2007, 3462, 1012, 2070, 22262, 5055, 2031, 5967, 2000, 2224, 5250, 102, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]], 'offset_mapping': [[(0, 0), (0, 4), (5, 9), (10, 15), (16, 20), (21, 27), (27, 28), (0, 0), (0, 4), (5, 8), (9, 17), (17, 18), (19, 24), (25, 28), (29, 38), (39, 41), (41, 44), (44, 47), (47, 49), (49, 50), (51, 55), (56, 58), (58, 59), (60, 65), (66, 72), (72, 73), (74, 81), (82, 90), (90, 93), (94, 99), (100, 104), (105, 110), (111, 116), (116, 122), (123, 126), (127, 129), (129, 132), (132, 134), (135, 137), (138, 140), (141, 143), (143, 145), (146, 150), (151, 158), (159, 161), (162, 164), (164, 166), (167, 169), (170, 177), (178, 185), (186, 189), (190, 192), (192, 196), (196, 197), (198, 202), (203, 206), (207, 210), (210, 213), (213, 216), (216, 217), (218, 223), (224, 227), (227, 228), (228, 229), (230, 234), (235, 236), (237, 239), (239, 242), (242, 244), (245, 252), (253, 255), (256, 264), (265, 267), (267, 270), (270, 273), (274, 281), (282, 285), (286, 287), (288, 292), (293, 302), (303, 305), (306, 309), (310, 312), (312, 316), (316, 317), (318, 319), (320, 322), (322, 324), (325, 329), (330, 332), (333, 335), (335, 338), (338, 341), (342, 347), (348, 352), (353, 355), (355, 358), (359, 361), (362, 363), (364, 368), (368, 371), (371, 373), (374, 379), (379, 380), (381, 388), (388, 389), (390, 395), (396, 400), (401, 403), (404, 411), (411, 416), (417, 420), (421, 423), (424, 426), (426, 428), (428, 431), (431, 437), (438, 440), (440, 443), (443, 446), (446, 449), (449, 450), (451, 453), (453, 456), (456, 460), (461, 465), (466, 468), (469, 472), (473, 481), (481, 484), (485, 490), (490, 491), (492, 494), (495, 502), (502, 503), (504, 508), (509, 513), (514, 516), (516, 519), (519, 521), (522, 524), (524, 526), (526, 530), (530, 531), (532, 538), (539, 543), (544, 546), (546, 548), (548, 551), (551, 554), (555, 559), (560, 567), (567, 568), (569, 573), (574, 582), (582, 583), (584, 586), (587, 591), (592, 594), (595, 598), (599, 605), (606, 608), (609, 612), (613, 616), (616, 619), (619, 623), (623, 624), (625, 632), (633, 637), (638, 641), (642, 646), (646, 647), (647, 648), (649, 651), (651, 653), (653, 655), (655, 656), (657, 660), (661, 663), (663, 665), (665, 667), (668, 670), (671, 672), (673, 678), (679, 680), (681, 688), (689, 696), (697, 698), (699, 704), (705, 707), (708, 716), (717, 724), (725, 727), (727, 728), (729, 733), (734, 739), (740, 744), (745, 747), (748, 755), (756, 758), (758, 760), (760, 762), (762, 763), (764, 767), (768, 775), (776, 779), (780, 784), (785, 789), (790, 792), (792, 793), (794, 796), (797, 805), (805, 806), (807, 811), (812, 819), (820, 822), (823, 828), (829, 832), (832, 835), (835, 840), (841, 843), (843, 847), (847, 848), (848, 849), (850, 855), (856, 862), (863, 866), (866, 868), (868, 870), (870, 874), (874, 876), (877, 878), (879, 883), (884, 887), (888, 897), (898, 900), (901, 904), (905, 907), (907, 909), (909, 910), (911, 912), (912, 913), (914, 917), (918, 920), (920, 923), (923, 928), (928, 930), (931, 932), (933, 937), (938, 941), (942, 951), (952, 954), (955, 961), (961, 964), (965, 966), (966, 967), (968, 971), (972, 974), (975, 977), (977, 983), (983, 986), (987, 992), (993, 995), (996, 1000), (1000, 1005), (1005, 1007), (1008, 1009), (1010, 1013), (1014, 1019), (1020, 1029), (1030, 1032), (1033, 1035), (1035, 1037), (1037, 1041), (1042, 1043), (1044, 1051), (1052, 1053), (1054, 1059), (1059, 1060), (1061, 1066), (1067, 1069), (1070, 1075), (1076, 1083), (1084, 1086), (1087, 1090), (1090, 1094), (1094, 1095), (1096, 1099), (1100, 1106), (1107, 1109), (1110, 1117), (1118, 1120), (1121, 1123), (1124, 1131), (1132, 1134), (1135, 1140), (1141, 1152), (1152, 1153), (1154, 1158), (1159, 1162), (1163, 1166), (1166, 1173), (1173, 1174), (1175, 1177), (1178, 1180), (1181, 1187), (1188, 1194), (1195, 1198), (1199, 1202), (1202, 1204), (1204, 1207), (1207, 1209), (1210, 1221), (1222, 1228), (1229, 1232), (1233, 1235), (1235, 1237), (1237, 1239), (1239, 1240), (1241, 1245), (1246, 1252), (1253, 1256), (1257, 1261), (1261, 1262), (1263, 1266), (1267, 1273), (1273, 1276), (1277, 1283), (1284, 1286), (1287, 1292), (1293, 1295), (1296, 1302), (1302, 1303), (1304, 1308), (1309, 1310), (1311, 1315), (1316, 1319), (1320, 1327), (1328, 1331), (1332, 1333), (1334, 1336), (1336, 1339), (1339, 1341), (1342, 1346), (1347, 1355), (1356, 1365), (1366, 1372), (1373, 1376), (1377, 1385), (1386, 1390), (1391, 1393), (1394, 1404), (1405, 1408), (1409, 1412), (1413, 1417), (1418, 1420), (1421, 1426), (1426, 1427), (1428, 1432), (1433, 1438), (1439, 1442), (1443, 1449), (1450, 1457), (1458, 1461), (1462, 1467), (1468, 1474), (1474, 1477), (1478, 1480), (1481, 1484), (1485, 1489), (1490, 1496), (1496, 1497), (1498, 1502), (1503, 1512), (1513, 1518), (1519, 1523), (1524, 1531), (1532, 1534), (1535, 1538), (1539, 1546), (0, 0), (0, 0), (0, 0), (0, 0)]], 'overflow_to_sample_mapping': [0]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tttt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tttt[\"input_ids\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraphs_mapper[\"418_19_57312110a5e9cc1400cdbc57_0\"].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and in rudimentary forms in galliformes ( but fully developed in cracidae ) possess a penis, which is never present in neoaves. the length is thought to be related to sperm competition. when not copulating, it is hidden within the proctodeum compartment within the cloaca, just inside the vent. the digestive system of birds is unique, with a crop for storage and a gizzard that contains swallowed stones for grinding food to compensate for the lack of teeth. most birds are highly adapted for rapid digestion to aid with flight. some migratory birds have adapted to use protein from many parts of their bodies, including protein from the intestines, as additional energy during migration.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs_mapper[\"418_19_57312110a5e9cc1400cdbc57_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paragraph_id              418_19_57312110a5e9cc1400cdbc57_0\n",
       "question_id                        57312110a5e9cc1400cdbc57\n",
       "answer_id                                                 0\n",
       "answer_start                                           1452\n",
       "answer_text                                 rapid digestion\n",
       "question_text                  What aids birds with flight?\n",
       "tokenizer_answer_start                                  363\n",
       "tokenizer_answer_end                                    366\n",
       "Name: 83128, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[83128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#failed['input_ids'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for failed_id in failed['input_ids']:\n",
    "    print(len(failed_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 1 at dim 1 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-944d3cde853d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 1 at dim 1 (got 2)"
     ]
    }
   ],
   "source": [
    "torch.tensor(failed['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, lr: 0.01, Train loss: 11.9244,  Train acc start: 0.0037, Train acc end: 0.0015, Time: 2007.7466\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e57684057a3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m#eval_results = evaluate_model_on_data(model, evaluator, train_data_loader, paragraphs_mapper, device, debug=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcur_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-a0a5b9949b1f>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, optimizer, loss_function, dataloader, device)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_answer_start_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_spans_start\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_answer_end_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_spans_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Optimizer step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/squad_qa_pytorch/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/squad_qa_pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = {\"train_loss\": [], \"train_acc_start\": [], \"train_acc_end\": []}\n",
    "loop_start = timer()\n",
    "# lr scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5, threshold=0.01)\n",
    "for epoch in range(50):\n",
    "    train_dict = train_step(model, optimizer, loss_function, train_data_loader, device=device)\n",
    "    #eval_results = evaluate_model_on_data(model, evaluator, train_data_loader, paragraphs_mapper, device, debug=True)\n",
    "    cur_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'Epoch: {epoch}, lr: {cur_lr}, Train loss: {train_dict[\"loss\"]:.4f},  Train acc start: {train_dict[\"accuracy_start\"]:.4f}, Train acc end: {train_dict[\"accuracy_end\"]:.4f}, Time: {train_dict[\"time\"]:.4f}')\n",
    "    history[\"train_loss\"].append(train_dict[\"loss\"]);history[\"train_acc_start\"].append(train_dict[\"accuracy_start\"]);history[\"train_acc_end\"].append(train_dict[\"accuracy_end\"]);\n",
    "    #history[\"val_loss\"].append(val_dict[\"loss\"]);history[\"val_acc\"].append(val_dict[\"accuracy\"]);\n",
    "    #scheduler.step(val_dict[\"loss\"])\n",
    "    #print(f\"Evaluation Results: {eval_results}\")\n",
    "loop_end = timer()\n",
    "print(f\"Elapsed time: {(loop_end - loop_start):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNOTE: this logic is used for sample creation only, such that each sample is \"short enough\" for BERT; \\n      a duplicate of this logic will need to be used in QADataset Dataloader class when we\\'ll take\\n      short samples\\' text, tokenize them again, and find the correct index\\nALTERNATIVE: for BERT models we could directly get the answer spans, and pass them in dataframe to another QADataset\\n             built specifically for BERT, that will just take the data from dataframe (way nicer and faster solution).\\nSUGGESTION: we could also use specific dict keys and in QADataset pick stuff from these keys: \\n                - if these keys are absent then don\\'t use BERT logic (eg span_start and span_end) and use previous logic\\n                - if these keys are present, then just use them and gather the BERT samples.\\n                Call these keys like \"tokenizer_span_idx\" (to make them kinda unique)\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTE: this logic is used for sample creation only, such that each sample is \"short enough\" for BERT; \n",
    "      a duplicate of this logic will need to be used in QADataset Dataloader class when we'll take\n",
    "      short samples' text, tokenize them again, and find the correct index\n",
    "ALTERNATIVE: for BERT models we could directly get the answer spans, and pass them in dataframe to another QADataset\n",
    "             built specifically for BERT, that will just take the data from dataframe (way nicer and faster solution).\n",
    "SUGGESTION: we could also use specific dict keys and in QADataset pick stuff from these keys: \n",
    "                - if these keys are absent then don't use BERT logic (eg span_start and span_end) and use previous logic\n",
    "                - if these keys are present, then just use them and gather the BERT samples.\n",
    "                Call these keys like \"tokenizer_span_idx\" (to make them kinda unique)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "x-y1GLEZJPvA",
    "outputId": "20f8a400-7f2a-406f-cd8d-87dd5698559d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>question_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_0</td>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>0</td>\n",
       "      <td>515</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_0</td>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_0</td>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>0</td>\n",
       "      <td>279</td>\n",
       "      <td>the Main Building</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0_0</td>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>0</td>\n",
       "      <td>381</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_0</td>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  paragraph_id               question_id  answer_id  answer_start  \\\n",
       "0          0_0  5733be284776f41900661182          0           515   \n",
       "1          0_0  5733be284776f4190066117f          0           188   \n",
       "2          0_0  5733be284776f41900661180          0           279   \n",
       "3          0_0  5733be284776f41900661181          0           381   \n",
       "4          0_0  5733be284776f4190066117e          0            92   \n",
       "\n",
       "                               answer_text  \\\n",
       "0               Saint Bernadette Soubirous   \n",
       "1                a copper statue of Christ   \n",
       "2                        the Main Building   \n",
       "3  a Marian place of prayer and reflection   \n",
       "4       a golden statue of the Virgin Mary   \n",
       "\n",
       "                                       question_text  \n",
       "0  To whom did the Virgin Mary allegedly appear i...  \n",
       "1  What is in front of the Notre Dame Main Building?  \n",
       "2  The Basilica of the Sacred heart at Notre Dame...  \n",
       "3                  What is the Grotto at Notre Dame?  \n",
       "4  What sits on top of the Main Building at Notre...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs_mapper, df = build_mappers_and_dataframe(data, limit_answers=1)\n",
    "print(paragraphs_mapper[next(iter(paragraphs_mapper))])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text_dict: Dict[str, Any], text_key: Union[str, None] = None) -> Any:\n",
    "    text_dict = copy.deepcopy(text_dict)\n",
    "    # just tokenize and remove punctuation for now\n",
    "    # TODO: add better punctuation removal later\n",
    "    tokenizer = SpaceTokenizer()#TreebankWordTokenizer()\n",
    "    for key in text_dict.keys():\n",
    "        if text_key is not None:\n",
    "            text = tokenizer.tokenize(text_dict[key][text_key])\n",
    "            text_dict[key][text_key] = text\n",
    "        else:\n",
    "            text = tokenizer.tokenize(text_dict[key])\n",
    "            text_dict[key] = text\n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_mapper = preprocess_text(paragraphs_mapper)\n",
    "df['question_text'] = df.apply(lambda row: nltk.word_tokenize(row['question_text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5hpoaGpK4qYe"
   },
   "outputs": [],
   "source": [
    "# Extend the paragraphs mapper to include spans\n",
    "paragraphs_spans_mapper = add_paragraphs_spans(paragraphs_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E02_XWu_4qYe",
    "outputId": "9d318e81-3fac-4d9c-b214-865708f5c71d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Architecturally,', 'the', 'school', 'has', 'a', 'Catholic', 'character.', 'Atop', 'the', 'Main', \"Building's\", 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'Virgin', 'Mary.', 'Immediately', 'in', 'front', 'of', 'the', 'Main', 'Building', 'and', 'facing', 'it,', 'is', 'a', 'copper', 'statue', 'of', 'Christ', 'with', 'arms', 'upraised', 'with', 'the', 'legend', '\"Venite', 'Ad', 'Me', 'Omnes\".', 'Next', 'to', 'the', 'Main', 'Building', 'is', 'the', 'Basilica', 'of', 'the', 'Sacred', 'Heart.', 'Immediately', 'behind', 'the', 'basilica', 'is', 'the', 'Grotto,', 'a', 'Marian', 'place', 'of', 'prayer', 'and', 'reflection.', 'It', 'is', 'a', 'replica', 'of', 'the', 'grotto', 'at', 'Lourdes,', 'France', 'where', 'the', 'Virgin', 'Mary', 'reputedly', 'appeared', 'to', 'Saint', 'Bernadette', 'Soubirous', 'in', '1858.', 'At', 'the', 'end', 'of', 'the', 'main', 'drive', '(and', 'in', 'a', 'direct', 'line', 'that', 'connects', 'through', '3', 'statues', 'and', 'the', 'Gold', 'Dome),', 'is', 'a', 'simple,', 'modern', 'stone', 'statue', 'of', 'Mary.']\n",
      "[(0, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 53), (54, 58), (59, 62), (63, 67), (68, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 232), (233, 237), (238, 241), (242, 248), (249, 256), (257, 259), (260, 262), (263, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 333), (334, 345), (346, 352), (353, 356), (357, 365), (366, 368), (369, 372), (373, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 451), (452, 454), (455, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 502), (503, 511), (512, 514), (515, 520), (521, 531), (532, 541), (542, 544), (545, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 653), (654, 656), (657, 658), (659, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 695)]\n"
     ]
    }
   ],
   "source": [
    "print(paragraphs_spans_mapper['0_0']['text'])\n",
    "print(paragraphs_spans_mapper['0_0']['spans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87599"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xb4YK_Qa95zK"
   },
   "source": [
    "### DataConverter and CustomQADataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KDl4CIW-mj_D",
    "outputId": "f012b0d4-f66e-4917-c208-1ebebe9a2f23"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paragraphs_spans_mapper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-fb41f6d7a499>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_loading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqa_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCustomQADataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata_converter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataConverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparagraphs_spans_mapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdatasetQA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomQADataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_converter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparagraphs_mapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasetQA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadder_collate_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'paragraphs_spans_mapper' is not defined"
     ]
    }
   ],
   "source": [
    "from data_loading.utils import DataConverter, padder_collate_fn\n",
    "from data_loading.qa_dataset import CustomQADataset\n",
    "\n",
    "data_converter = DataConverter(embedding_model, paragraphs_spans_mapper)\n",
    "datasetQA = CustomQADataset(data_converter, df, paragraphs_mapper)\n",
    "data_loader = torch.utils.data.DataLoader(datasetQA, collate_fn = padder_collate_fn, batch_size=10, shuffle=True)\n",
    "\n",
    "test_batch = next(iter(data_loader))\n",
    "print(test_batch[\"paragraph_emb\"].shape)\n",
    "print(test_batch[\"y_gt\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from models.utils import SpanExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"The device is {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model:\n",
    "\n",
    "(paragraph_emb, question_emb) -> (answer_start, answer_end) // for each token in paragraph_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, loss_function, dataloader, device=\"cpu\"):\n",
    "    acc_loss = 0\n",
    "    acc_start_accuracy = 0\n",
    "    acc_end_accuracy = 0\n",
    "    count = 0\n",
    "\n",
    "    time_start = timer()\n",
    "    \n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        input_ids_in = batch[\"input_ids\"]\n",
    "        atten_mask_in = batch[\"attention_mask\"]\n",
    "        answer_spans_start = batch[\"y_gt\"][:, 0]\n",
    "        answer_spans_end = batch[\"y_gt\"][:, 1]\n",
    "        # Clear gradients\n",
    "        model.zero_grad()\n",
    "        # Place to right device\n",
    "        input_ids_in = input_ids_in.to(device)\n",
    "        atten_mask_in = atten_mask_in.to(device)\n",
    "        answer_spans_start = answer_spans_start.to(device)\n",
    "        answer_spans_end = answer_spans_end.to(device)\n",
    "        # Run forward pass\n",
    "        pred_answer_start_scores, pred_answer_end_scores = model(input_ids_in, atten_mask_in)\n",
    "        # Compute the CrossEntropyLoss\n",
    "        loss = loss_function(pred_answer_start_scores, answer_spans_start) + loss_function(pred_answer_end_scores, answer_spans_end)\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        # --- Compute metrics ---\n",
    "        # Get span indexes\n",
    "        pred_span_start_idxs, pred_span_end_idxs = SpanExtractor.extract_most_probable(pred_answer_start_scores, pred_answer_end_scores)\n",
    "        gt_start_idxs = answer_spans_start.cpu().detach()\n",
    "        gt_end_idxs = answer_spans_end.cpu().detach()\n",
    "        # two accs\n",
    "        start_accuracy = torch.sum(gt_start_idxs == pred_span_start_idxs) / len(pred_span_start_idxs)\n",
    "        end_accuracy = torch.sum(gt_end_idxs == pred_span_end_idxs) / len(pred_span_end_idxs)\n",
    "        # Gather stats\n",
    "        acc_loss += loss.item()\n",
    "        acc_start_accuracy += start_accuracy.item()\n",
    "        acc_end_accuracy += end_accuracy.item()\n",
    "        count += 1\n",
    "    time_end = timer()\n",
    "    return {\n",
    "        \"loss\": acc_loss / count, \n",
    "        \"accuracy_start\": acc_start_accuracy / count, \n",
    "        \"accuracy_end\": acc_end_accuracy / count,\n",
    "        \"time\": time_end - time_start\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Evaluator object\n",
    "evaluator = Evaluator(documents_list=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_data(model, evaluator, dataloader, paragraphs_mapper, device, debug=False):\n",
    "    eval_dict = build_evaluation_dict(model, dataloader, paragraphs_mapper, device)\n",
    "    if debug:\n",
    "        print(f\"DEBUG: Eval_dict: {eval_dict}\")\n",
    "    stats = {}\n",
    "    stats['exact_match'] = evaluator.ExactMatch(eval_dict)\n",
    "    stats['f1'] = evaluator.F1(eval_dict)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSum(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"\n",
    "        General idea, given a random dummy weights vector, \n",
    "        learn to weight it based on query\n",
    "        \"\"\"\n",
    "        super(WeightedSum, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(input_dim))\n",
    "\n",
    "    def forward(self, input_emb, mask=None):\n",
    "        # TODO: if needed, implement time masking\n",
    "        batch, timesteps, embed_dim = input_emb.shape\n",
    "        # w dot q_j\n",
    "        dot_prods = torch.matmul(input_emb, self.weights)\n",
    "        # exp(w dot q_j)\n",
    "        exp_prods = torch.exp(dot_prods)\n",
    "        # normalization factor\n",
    "        sum_exp_prods = torch.sum(exp_prods, dim=1)\n",
    "        sum_exp_prods = sum_exp_prods.repeat(timesteps, 1).T\n",
    "        # b_j\n",
    "        b = exp_prods / sum_exp_prods\n",
    "        # q (embedding) = sum_t(b_t * q_t)\n",
    "        b_scal_q = input_emb * b[:, :, None]\n",
    "        # now sum along correct axis\n",
    "        q = torch.sum(b_scal_q, axis=1)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compatibility functions**\n",
    "\n",
    "**Multiplicative (dot)**:\n",
    "\n",
    "p = paragraph emb shape: [B, T, E] (Query)\n",
    "\n",
    "q = question weighted shape: [B, E] reshaped to [B, E, 1] (Keys)\n",
    "\n",
    "scores = p @ q (of shape: [B, T, 1])\n",
    "\n",
    "**General bilinear**:\n",
    "\n",
    "p = paragraph emb shape: [B, T, Ep] (Query)\n",
    "\n",
    "q = question weighted shape: [B, Eq] reshaped to [B, Eq, 1] (Keys)\n",
    "\n",
    "W = parameter matrix of shape: [Ep, Eq]\n",
    "\n",
    "scores = p @ W @ q (of shape: [B, T, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearCompatibility(nn.Module):\n",
    "    def __init__(self, query_dim, keys_dim):\n",
    "        \"\"\"\n",
    "        Perform bilinear compatibility f(q, K) = q.T @ W @ K\n",
    "        Recall: multiplicative/dot compatibility is f(q, K) = q.T @ K\n",
    "        \n",
    "        Where: \n",
    "            q -> embedded paragraphs (p in DrQA)\n",
    "            K -> embedded question (q in DrQA)\n",
    "        \"\"\"\n",
    "        super(BilinearCompatibility, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(query_dim, keys_dim))\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        \"\"\"\n",
    "        query: batch of shape (batch, seq_len, query_dim) (Query)\n",
    "        keys = batch of shape (batch, key_dim) which will be reshaped into [batch, key_dim, 1] (Keys)\n",
    "        \"\"\"\n",
    "        return query @ self.weights @ keys[:, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_QA(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\n",
    "        super(LSTM_QA, self).__init__()\n",
    "        self.tagset_size = tagset_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.paragraph_embedder = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.question_embedder = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.weighted_sum = WeightedSum(hidden_dim * 2)\n",
    "        # used to compute similarity scores\n",
    "        self.general_bilinear_start = BilinearCompatibility(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.general_bilinear_end = BilinearCompatibility(hidden_dim * 2, hidden_dim * 2)\n",
    "        # to classify from similarity to prob of start and prob of end\n",
    "        self.sim_to_start = nn.Linear(1, 1) # given a similarity score, predict P(start)\n",
    "        self.sim_to_end = nn.Linear(1, 1) # given a similarity score, predict P(end)\n",
    "\n",
    "    def forward(self, paragraphs, questions):\n",
    "        batch_size, seq_len, n_feat = paragraphs.shape\n",
    "        # As we assume batch_first true, then our sentence_embeddings will have correct shape\n",
    "        paragraphs_seq_emb, _ = self.paragraph_embedder(paragraphs) # (batch, seq_len, n_feats * n_dirs)\n",
    "        questions_seq_emb, _ = self.question_embedder(questions) # (batch, seq_len, n_feats * n_dirs)\n",
    "        # weighted sum\n",
    "        questions_state_repr = self.weighted_sum(questions_seq_emb)\n",
    "        #return paragraphs_seq_emb, questions_state_repr\n",
    "        # compute similarities -> (batch, timestep, 1)\n",
    "        similarities_start = self.general_bilinear_start(paragraphs_seq_emb, questions_state_repr)\n",
    "        similarities_end = self.general_bilinear_start(paragraphs_seq_emb, questions_state_repr)\n",
    "        #print(f\"INSIDE MODEL: similarities shape: {similarities.shape}\") #DEBUG\n",
    "        # --- Given a similarity score, predict P(start), P(end) ---\n",
    "        # similarities flattened\n",
    "        similarities_start = similarities_start.contiguous()\n",
    "        similarities_start = similarities_start.view(-1, 1) # as similarity dim is 1 -> viewed shape is (batch*timestep, 1)\n",
    "        start_scores = self.sim_to_start(similarities_start)\n",
    "        start_logits = start_scores.view(batch_size, seq_len) # P(start)\n",
    "        \n",
    "        similarities_end = similarities_end.contiguous()\n",
    "        similarities_end = similarities_end.view(-1, 1) # as similarity dim is 1 -> viewed shape is (batch*timestep, 1)\n",
    "        end_scores = self.sim_to_end(similarities_end)\n",
    "        end_logits = end_scores.view(batch_size, seq_len) # P(end)\n",
    "        \n",
    "        # if we view each sequence of tokens as a feature vector\n",
    "        # we can interpret the start/end assignation problem as \n",
    "        # a classification with a variable number of classes\n",
    "        # thus assume that our model outputs logits that will just be passed\n",
    "        # to a softmax, to build a probable distribution of the start token\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.nn.functional.softmax(outs_mod[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline model\n",
    "model = LSTM_QA(embedding_model.vector_size, 128, 10).to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_converter = DataConverter(embedding_model, paragraphs_spans_mapper)\n",
    "datasetQA = CustomQADataset(data_converter, df, paragraphs_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(datasetQA, collate_fn = padder_collate_fn, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Might fail ['When', 'the', 'news', 'arrived', 'in', 'England', 'it', 'caused', 'an', 'outcry.', 'In', 'response,', 'a', 'combined', 'bounty', 'of', 'Â£1,000', 'was', 'offered', 'for', \"Every's\", 'capture', 'by', 'the', 'Privy', 'Council', 'and', 'East', 'India', 'Company,', 'leading', 'to', 'the', 'first', 'worldwide', 'manhunt', 'in', 'recorded', 'history.', 'The', 'plunder', 'of', \"Aurangzeb's\", 'treasure', 'ship', 'had', 'serious', 'consequences', 'for', 'the', 'English', 'East', 'India', 'Company.', 'The', 'furious', 'Mughal', 'Emperor', 'Aurangzeb', 'ordered', 'Sidi', 'Yaqub', 'and', 'Nawab', 'Daud', 'Khan', 'to', 'attack', 'and', 'close', 'four', 'of', 'the', \"company's\", 'factories', 'in', 'India', 'and', 'imprison', 'their', 'officers,', 'who', 'were', 'almost', 'lynched', 'by', 'a', 'mob', 'of', 'angry', 'Mughals,', 'blaming', 'them', 'for', 'their', \"countryman's\", 'depredations,', 'and', 'threatened', 'to', 'put', 'an', 'end', 'to', 'all', 'English', 'trading', 'in', 'India.', 'To', 'appease', 'Emperor', 'Aurangzeb', 'and', 'particularly', 'his', 'Grand', 'Vizier', 'Asad', 'Khan,', 'Parliament', 'exempted', 'Every', 'from', 'all', 'of', 'the', 'Acts', 'of', 'Grace', '(pardons)', 'and', 'amnesties', 'it', 'would', 'subsequently', 'issue', 'to', 'other', 'pirates.'] len 140 start tensor(327)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, lr: 0.01, Train loss: 8.3310,  Train acc start: 0.0423, Train acc end: 0.0603, Time: 398.3827\n",
      "Evaluation Results: {'exact_match': 5.293439422824462, 'f1': 9.209568529959142}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-9ac0f04a38e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model_on_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparagraphs_mapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcur_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-f4220eca1b82>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, optimizer, loss_function, dataloader, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mparagraph_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"paragraph_emb\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mquestion_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question_emb\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/squad_qa_pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/squad_qa_pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/squad_qa_pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/squad_qa_pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/uni/magistrale_ai/secondo_anno/nlp/project/SQuAD-QA/data_loading/qa_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mparagraph_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparagraphs_mapper\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparagraph_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mparagraph_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_converter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_sequence_to_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraph_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mquestion_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_converter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_sequence_to_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/uni/magistrale_ai/secondo_anno/nlp/project/SQuAD-QA/data_loading/utils.py\u001b[0m in \u001b[0;36mword_sequence_to_embedding\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moovs_memory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparagraph_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_start\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_text\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = {\"train_loss\": [], \"train_acc_start\": [], \"train_acc_end\": []}\n",
    "loop_start = timer()\n",
    "# lr scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5, threshold=0.01)\n",
    "for epoch in range(50):\n",
    "    train_dict = train_step(model, optimizer, loss_function, train_data_loader, device=device)\n",
    "    eval_results = evaluate_model_on_data(model, evaluator, train_data_loader, paragraphs_mapper, device, debug=True)\n",
    "    cur_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'Epoch: {epoch}, lr: {cur_lr}, Train loss: {train_dict[\"loss\"]:.4f},  Train acc start: {train_dict[\"accuracy_start\"]:.4f}, Train acc end: {train_dict[\"accuracy_end\"]:.4f}, Time: {train_dict[\"time\"]:.4f}')\n",
    "    history[\"train_loss\"].append(train_dict[\"loss\"]);history[\"train_acc_start\"].append(train_dict[\"accuracy_start\"]);history[\"train_acc_end\"].append(train_dict[\"accuracy_end\"]);\n",
    "    #history[\"val_loss\"].append(val_dict[\"loss\"]);history[\"val_acc\"].append(val_dict[\"accuracy\"]);\n",
    "    #scheduler.step(val_dict[\"loss\"])\n",
    "    print(f\"Evaluation Results: {eval_results}\")\n",
    "loop_end = timer()\n",
    "print(f\"Elapsed time: {(loop_end - loop_start):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "NLP_Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "squad_qa_pytorch",
   "language": "python",
   "name": "squad_qa_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
