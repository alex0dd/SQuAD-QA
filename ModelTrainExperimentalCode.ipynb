{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/alexpod1000/SQuAD-QA/blob/main/ModelTrainExperimentalCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oTkVfFrJ-pzG"
   },
   "outputs": [],
   "source": [
    "#%%bash\n",
    "#[[ ! -e /colabtools ]] && exit  # Continue only if running on Google Colab\n",
    "\n",
    "# Clone repository\n",
    "# https://sysadmins.co.za/clone-a-private-github-repo-with-personal-access-token/\n",
    "# For cloning the main branch:\n",
    "#!git clone https://fb5b65b126107273e595ce8b6c9d2d533103c6e2:x-oauth-basic@github.com/alexpod1000/SQuAD-QA.git\n",
    "# For cloning the \"evaluation-features\" branch\n",
    "#!git clone --branch evaluation-features https://fb5b65b126107273e595ce8b6c9d2d533103c6e2:x-oauth-basic@github.com/alexpod1000/SQuAD-QA.git\n",
    "# Change current working directory to match project\n",
    "#%cd SQuAD-QA/\n",
    "#!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rsBVuJu6_5qN"
   },
   "outputs": [],
   "source": [
    "# External imports\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import torch\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer, SpaceTokenizer\n",
    "from typing import Tuple, List, Dict, Any, Union\n",
    "\n",
    "# Project imports\n",
    "from squad_data.parser import SquadFileParser\n",
    "from squad_data.utils import build_mappers_and_dataframe, add_paragraphs_spans\n",
    "from evaluation.evaluation_metrics import Evaluator\n",
    "from evaluation.utils import extract_answer, build_evaluation_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "387a021D9piE"
   },
   "source": [
    "### Download Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xFweP2uIJg8O",
    "outputId": "7bb5b9ca-7b89-4fad-dc90-c979f6ef5f0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-downloaded embeddings from /home/giulio/Documenti/ProgettiGIT/SQuAD-QA/embedding_models/embedding_model.kv\n",
      "End!\n",
      "Embedding dimension: 50\n"
     ]
    }
   ],
   "source": [
    "from utils.embedding_utils import EmbeddingDownloader\n",
    "\n",
    "embedding_downloader = EmbeddingDownloader(\n",
    "    \"embedding_models\", \n",
    "    \"embedding_model.kv\", \n",
    "    model_name=\"fasttext-wiki-news-subwords-300\"\n",
    ")\n",
    "\n",
    "embedding_model = embedding_downloader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Rh4dSW-9tYm"
   },
   "source": [
    "### Parse the json and get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FAEEYoypAOKA"
   },
   "outputs": [],
   "source": [
    "parser = SquadFileParser(\"squad_data/data/training_set.json\")\n",
    "data = parser.parse_documents()\n",
    "\n",
    "########################### DEBUG\n",
    "# reduce size for faster testing\n",
    "#full_data = data\n",
    "#data = []\n",
    "#for i in range(1): # use only the first 1 documents\n",
    "#  data.append(full_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKK-4d1_93QE"
   },
   "source": [
    "### Prepare the mappers and datafram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "x-y1GLEZJPvA",
    "outputId": "20f8a400-7f2a-406f-cd8d-87dd5698559d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_0</td>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>0</td>\n",
       "      <td>515</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_0</td>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_0</td>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>0</td>\n",
       "      <td>279</td>\n",
       "      <td>the Main Building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0_0</td>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>0</td>\n",
       "      <td>381</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_0</td>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  paragraph_id               question_id  answer_id  answer_start  \\\n",
       "0          0_0  5733be284776f41900661182          0           515   \n",
       "1          0_0  5733be284776f4190066117f          0           188   \n",
       "2          0_0  5733be284776f41900661180          0           279   \n",
       "3          0_0  5733be284776f41900661181          0           381   \n",
       "4          0_0  5733be284776f4190066117e          0            92   \n",
       "\n",
       "                               answer_text  \n",
       "0               Saint Bernadette Soubirous  \n",
       "1                a copper statue of Christ  \n",
       "2                        the Main Building  \n",
       "3  a Marian place of prayer and reflection  \n",
       "4       a golden statue of the Virgin Mary  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs_mapper, questions_mapper, df = build_mappers_and_dataframe(data)\n",
    "print(questions_mapper[next(iter(questions_mapper))])\n",
    "print(paragraphs_mapper[next(iter(paragraphs_mapper))])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text_dict: Dict[str, Any], text_key: Union[str, None] = None) -> Any:\n",
    "    text_dict = copy.deepcopy(text_dict)\n",
    "    # just tokenize and remove punctuation for now\n",
    "    # TODO: add better punctuation removal later\n",
    "    tokenizer = SpaceTokenizer()#TreebankWordTokenizer()\n",
    "    for key in text_dict.keys():\n",
    "        if text_key is not None:\n",
    "            text = tokenizer.tokenize(text_dict[key][text_key])\n",
    "            text_dict[key][text_key] = text\n",
    "        else:\n",
    "            text = tokenizer.tokenize(text_dict[key])\n",
    "            text_dict[key] = text\n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_mapper = preprocess_text(paragraphs_mapper)\n",
    "questions_mapper = preprocess_text(questions_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5hpoaGpK4qYe"
   },
   "outputs": [],
   "source": [
    "# Extend the paragraphs mapper to include spans\n",
    "paragraphs_spans_mapper = add_paragraphs_spans(paragraphs_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E02_XWu_4qYe",
    "outputId": "9d318e81-3fac-4d9c-b214-865708f5c71d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Architecturally,', 'the', 'school', 'has', 'a', 'Catholic', 'character.', 'Atop', 'the', 'Main', \"Building's\", 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'Virgin', 'Mary.', 'Immediately', 'in', 'front', 'of', 'the', 'Main', 'Building', 'and', 'facing', 'it,', 'is', 'a', 'copper', 'statue', 'of', 'Christ', 'with', 'arms', 'upraised', 'with', 'the', 'legend', '\"Venite', 'Ad', 'Me', 'Omnes\".', 'Next', 'to', 'the', 'Main', 'Building', 'is', 'the', 'Basilica', 'of', 'the', 'Sacred', 'Heart.', 'Immediately', 'behind', 'the', 'basilica', 'is', 'the', 'Grotto,', 'a', 'Marian', 'place', 'of', 'prayer', 'and', 'reflection.', 'It', 'is', 'a', 'replica', 'of', 'the', 'grotto', 'at', 'Lourdes,', 'France', 'where', 'the', 'Virgin', 'Mary', 'reputedly', 'appeared', 'to', 'Saint', 'Bernadette', 'Soubirous', 'in', '1858.', 'At', 'the', 'end', 'of', 'the', 'main', 'drive', '(and', 'in', 'a', 'direct', 'line', 'that', 'connects', 'through', '3', 'statues', 'and', 'the', 'Gold', 'Dome),', 'is', 'a', 'simple,', 'modern', 'stone', 'statue', 'of', 'Mary.']\n",
      "[(0, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 53), (54, 58), (59, 62), (63, 67), (68, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 232), (233, 237), (238, 241), (242, 248), (249, 256), (257, 259), (260, 262), (263, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 333), (334, 345), (346, 352), (353, 356), (357, 365), (366, 368), (369, 372), (373, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 451), (452, 454), (455, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 502), (503, 511), (512, 514), (515, 520), (521, 531), (532, 541), (542, 544), (545, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 653), (654, 656), (657, 658), (659, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 695)]\n"
     ]
    }
   ],
   "source": [
    "print(paragraphs_spans_mapper['0_0']['text'])\n",
    "print(paragraphs_spans_mapper['0_0']['spans'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xb4YK_Qa95zK"
   },
   "source": [
    "### DataConverter and CustomQADataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KDl4CIW-mj_D",
    "outputId": "f012b0d4-f66e-4917-c208-1ebebe9a2f23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 233, 50])\n",
      "torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "from data_loading.utils import DataConverter, padder_collate_fn\n",
    "from data_loading.qa_dataset import CustomQADataset\n",
    "\n",
    "data_converter = DataConverter(embedding_model, paragraphs_spans_mapper)\n",
    "datasetQA = CustomQADataset(data_converter, df, paragraphs_mapper, questions_mapper)\n",
    "data_loader = torch.utils.data.DataLoader(datasetQA, collate_fn = padder_collate_fn, batch_size=10, shuffle=True)\n",
    "\n",
    "test_batch = next(iter(data_loader))\n",
    "print(test_batch[\"paragraph_emb\"].shape)\n",
    "print(test_batch[\"y_gt\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giulio/anaconda3/envs/squad_qa_pytorch/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"The device is {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model:\n",
    "\n",
    "(paragraph_emb, question_emb) -> (answer_start, answer_end) // for each token in paragraph_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, loss_function, dataloader, device=\"cpu\"):\n",
    "    acc_loss = 0\n",
    "    acc_start_accuracy = 0\n",
    "    acc_end_accuracy = 0\n",
    "    count = 0\n",
    "\n",
    "    time_start = timer()\n",
    "    \n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        paragraph_in = batch[\"paragraph_emb\"]\n",
    "        question_in = batch[\"question_emb\"]\n",
    "        answer_spans_start = batch[\"y_gt\"][:, 0]\n",
    "        answer_spans_end = batch[\"y_gt\"][:, 1]\n",
    "        # Clear gradients\n",
    "        model.zero_grad()\n",
    "        # Place to right device\n",
    "        paragraph_in = paragraph_in.to(device)\n",
    "        question_in = question_in.to(device)\n",
    "        answer_spans_start = answer_spans_start.to(device)\n",
    "        answer_spans_end = answer_spans_end.to(device)\n",
    "        # Run forward pass\n",
    "        pred_answer_start_scores, pred_answer_end_scores = model(paragraph_in, question_in)\n",
    "        # Compute the CrossEntropyLoss\n",
    "        loss = loss_function(pred_answer_start_scores, answer_spans_start) + loss_function(pred_answer_end_scores, answer_spans_end)\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        # --- Compute metrics ---\n",
    "        # Get span indexes\n",
    "        pred_span_start_idxs = torch.argmax(pred_answer_start_scores, axis=-1).cpu().detach()\n",
    "        pred_span_end_idxs = torch.argmax(pred_answer_end_scores, axis=-1).cpu().detach()\n",
    "        gt_start_idxs = answer_spans_start.cpu().detach()\n",
    "        gt_end_idxs = answer_spans_end.cpu().detach()\n",
    "        # two accs\n",
    "        start_accuracy = torch.sum(gt_start_idxs == pred_span_start_idxs) / len(pred_span_start_idxs)\n",
    "        end_accuracy = torch.sum(gt_end_idxs == pred_span_end_idxs) / len(pred_span_end_idxs)\n",
    "        # Gather stats\n",
    "        acc_loss += loss.item()\n",
    "        acc_start_accuracy += start_accuracy.item()\n",
    "        acc_end_accuracy += end_accuracy.item()\n",
    "        count += 1\n",
    "    time_end = timer()\n",
    "    return {\n",
    "        \"loss\": acc_loss / count, \n",
    "        \"accuracy_start\": acc_start_accuracy / count, \n",
    "        \"accuracy_end\": acc_end_accuracy / count,\n",
    "        \"time\": time_end - time_start\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Evaluator object\n",
    "evaluator = Evaluator(documents_list=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_data(model, evaluator, dataloader, paragraphs_mapper, questions_mapper, device):\n",
    "    eval_dict = build_evaluation_dict(model, dataloader, paragraphs_mapper, questions_mapper, device)\n",
    "    print(f\"DEBUG: Eval_dict: {eval_dict}\")\n",
    "    stats = {}\n",
    "    stats['exact_match'] = evaluator.ExactMatch(eval_dict)\n",
    "    stats['f1'] = evaluator.F1(eval_dict)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSum(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"\n",
    "        General idea, given a random dummy weights vector, \n",
    "        learn to weight it based on query\n",
    "        \"\"\"\n",
    "        super(WeightedSum, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(input_dim))\n",
    "\n",
    "    def forward(self, input_emb, mask=None):\n",
    "        # TODO: if needed, implement time masking\n",
    "        batch, timesteps, embed_dim = input_emb.shape\n",
    "        # w dot q_j\n",
    "        dot_prods = torch.matmul(input_emb, self.weights)\n",
    "        # exp(w dot q_j)\n",
    "        exp_prods = torch.exp(dot_prods)\n",
    "        # normalization factor\n",
    "        sum_exp_prods = torch.sum(exp_prods, dim=1)\n",
    "        sum_exp_prods = sum_exp_prods.repeat(timesteps, 1).T\n",
    "        # b_j\n",
    "        b = exp_prods / sum_exp_prods\n",
    "        # q (embedding) = sum_t(b_t * q_t)\n",
    "        b_scal_q = input_emb * b[:, :, None]\n",
    "        # now sum along correct axis\n",
    "        q = torch.sum(b_scal_q, axis=1)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_QA(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\n",
    "        super(LSTM_QA, self).__init__()\n",
    "        self.tagset_size = tagset_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.paragraph_embedder = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.question_embedder = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.weighted_sum = WeightedSum(hidden_dim * 2)\n",
    "        # to classify from similarity to prob of start and prob of end\n",
    "        self.sim_to_prob = nn.Linear(1, 2) # given a similarity score, predict P(start), P(end)\n",
    "\n",
    "    def forward(self, paragraphs, questions):\n",
    "        batch_size, seq_len, n_feat = paragraphs.shape\n",
    "        # As we assume batch_first true, then our sentence_embeddings will have correct shape\n",
    "        paragraphs_seq_emb, _ = self.paragraph_embedder(paragraphs) # (batch, seq_len, n_feats * n_dirs)\n",
    "        questions_seq_emb, _ = self.question_embedder(questions) # (batch, seq_len, n_feats * n_dirs)\n",
    "        # weighted sum\n",
    "        questions_state_repr = self.weighted_sum(questions_seq_emb)\n",
    "        # compute similarities -> (batch, timestep, 1)\n",
    "        similarities = torch.bmm(paragraphs_seq_emb, questions_state_repr[:, :, None])\n",
    "        #print(f\"INSIDE MODEL: similarities shape: {similarities.shape}\") #DEBUG\n",
    "        # --- Given a similarity score, predict P(start), P(end) ---\n",
    "        # similarities flattened\n",
    "        similarities = similarities.contiguous()\n",
    "        similarities = similarities.view(-1, 1) # as similarity dim is 1 -> viewed shape is (batch*timestep, 1)\n",
    "        start_end_scores = self.sim_to_prob(similarities)\n",
    "        start_end_scores = start_end_scores.view(batch_size, seq_len, 2) # where 2 is (P(start), P(end))\n",
    "        \n",
    "        start_logits = start_end_scores[:, :, 0]\n",
    "        end_logits = start_end_scores[:, :, 1]\n",
    "        \n",
    "        # if we view each sequence of tokens as a feature vector\n",
    "        # we can interpret the start/end assignation problem as \n",
    "        # a classification with a variable number of classes\n",
    "        # thus assume that our model outputs logits that will just be passed\n",
    "        # to a softmax, to build a probable distribution of the start token\n",
    "        return start_logits, end_logits\n",
    "        \n",
    "        #self.sim_to_prob\n",
    "        \n",
    "        # we can for each similarity score predict just two scalars (simple 1 to 2 mapping NN), and use P(start)[idx_of_start] = 1, rest 0\n",
    "        # and P(end)[idx_of_end] = 1, rest 0 for the groundtruth\n",
    "        \n",
    "        # Crude question repr: take last state of lstm (remove padding), concat it (as it's a bilstm) and use it as question representation\n",
    "        # \n",
    "        #return paragraphs_seq_emb, questions_seq_emb, questions_state_repr, similarities\n",
    "        #\n",
    "        # THANK YOU DUDE: https://www.kdnuggets.com/2018/06/taming-lstms-variable-sized-mini-batches-pytorch.html\n",
    "        # Project to tag space\n",
    "        # Dim transformation: (batch_size, seq_len, nb_lstm_units) -> (batch_size * seq_len, nb_lstm_units)\n",
    "        # this one is a bit tricky as well. First we need to reshape the data so it goes into the linear layer\n",
    "        #\n",
    "        #lstm_out = lstm_out.contiguous()\n",
    "        #lstm_out = lstm_out.view(-1, lstm_out.shape[2])\n",
    "        #lstm_out = self.dropout(lstm_out)\n",
    "        # Run through actual linear layer\n",
    "        #tag_logits = self.hidden_to_tag(lstm_out)\n",
    "        # Dim transformation: (batch_size * seq_len, nb_lstm_units) -> (batch_size, seq_len, nb_tags)\n",
    "        #tag_logits = tag_logits.view(batch_size, seq_len, self.tagset_size)\n",
    "        #return tag_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.nn.functional.softmax(outs_mod[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline model\n",
    "model = LSTM_QA(embedding_model.vector_size, 128, 10).to(device)\n",
    "# NOTE: weight=torch.Tensor(class_weights_train).to(device) sucks badly, don't use it, it fucks up performance completly\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_converter = DataConverter(embedding_model, paragraphs_spans_mapper)\n",
    "datasetQA = CustomQADataset(data_converter, df, paragraphs_mapper, questions_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(datasetQA, collate_fn = padder_collate_fn, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Eval_dict: {'573398164776f41900660e21': 'Luigi', '5733b699d058e614000b6118': 'About', '573393184776f41900660da7': '$9.7', '573388ce4776f41900660cc6': 'regalia.', '573394c84776f41900660ddd': '(1987–2005),', '5733b3d64776f419006610a6': 'F.', '57339b36d058e614000b5ea6': '2012[update]', '57338a51d058e614000b5cf4': 'Father', '573387acd058e614000b5cb5': 'Division', '5733a6424776f41900660f4e': 'offered.', '573385394776f41900660c80': 'Father', '5733a70c4776f41900660f63': 'First', '5733cd504776f41900661290': 'F.', '5733c743d058e614000b622d': 'Army', '57339b36d058e614000b5ea4': '2012[update]', '5733bf84d058e614000b61c1': \"Mary's\", '5733ad384776f41900660fec': '14-story', '5733afd3d058e614000b6046': 'Best', '5733974d4776f41900660e17': '2005,', '573382a14776f41900660c2e': 'Notre', '5733c3184776f419006611c4': 'On', '57339a5bd058e614000b5e91': 'million', '573388ce4776f41900660cc5': '1924.', '5733cd504776f41900661292': 'basketball,', '5733974d4776f41900660e18': 'Jenkins,', '573387acd058e614000b5cb4': '105', '5733a4c54776f41900660f30': 'Father', '5733ccbe4776f41900661274': 'Airplane!', '5733926d4776f41900660d91': '(now', '5733afd3d058e614000b6049': 'Best', '5733b0fb4776f41900661041': 'Father', '5733ac31d058e614000b5ff6': 'Joan', '5733ccbe4776f41900661270': 'speech,', '57338724d058e614000b5c9e': 'Father', '5733c1a94776f419006611a6': 'Association', '5733c3184776f419006611c5': 'Notre', '573383e94776f41900660c5b': '29', '5733849bd058e614000b5c56': 'Hailandière,', '5733a3cbd058e614000b5f3f': 'Jesuit', '5733849bd058e614000b5c5a': 'Célestine', '5733caf74776f41900661250': 'teams,', '5733926d4776f41900660d90': 'Hall,', '5733cbdad058e614000b6290': 'F.', '57339a5bd058e614000b5e93': 'million', '57339a5bd058e614000b5e92': 'Guglielmino', '5733b496d058e614000b60cf': 'Jewish', '573383494776f41900660c41': 'Division', '5733c743d058e614000b622f': 'Army', '5733cd504776f4190066128e': 'basketball,', '5733a6424776f41900660f50': 'B.S.', '57338724d058e614000b5ca0': 'Jesuit', '5733849bd058e614000b5c58': 'Célestine', '5733b3d64776f419006610a4': 'Fitzsimons,', '5733a7bd4776f41900660f6c': 'program.', '5733b1da4776f41900661068': 'Father', '573393e1d058e614000b5dc4': 'Saint', '5733ac31d058e614000b5ff7': 'Joan', '5733cd504776f4190066128f': 'basketball,', '5733c3184776f419006611c6': 'Notre', '5733b5344776f419006610dd': 'Jenkins,', '5733caf74776f4190066124d': '1,600', '5733be284776f4190066117f': 'Main', '5733849bd058e614000b5c57': 'Hailandière,', '573394c84776f41900660ddf': 'million.', '573388ce4776f41900660cc3': 'Nativism', '5733b5df4776f41900661108': 'Around', '5733b496d058e614000b60ce': 'Jewish', '5733b7f74776f4190066112d': 'Jewish', '5733a55a4776f41900660f3c': 'Architecture', '5733be284776f41900661181': 'Main', '5733b1da4776f4190066106b': 'Father', '5733a7bd4776f41900660f6a': '(PhD)', '5733926d4776f41900660d8d': 'Hall,', '5733b7f74776f41900661131': 'Jewish', '5733afd3d058e614000b6048': 'Best', '573382a14776f41900660c31': 'Notre', '5733b496d058e614000b60d1': 'Jewish', '5733bf84d058e614000b61bd': 'Saint', '5733c4494776f419006611db': 'Ohio', '5733cbdad058e614000b628f': 'Shea,', '573398ebd058e614000b5e66': 'social,', '5733ccbe4776f41900661272': 'Rudy,', '5733bf84d058e614000b61c0': 'Saint', '5733b1da4776f4190066106a': 'Father', '573383494776f41900660c44': 'Heisman', '5733c743d058e614000b622e': '2012,', '5733c743d058e614000b6230': 'Army', '57338653d058e614000b5c83': 'Around', '5733c29c4776f419006611ba': 'Hockey', '573398164776f41900660e25': 'Luigi', '5733c4494776f419006611dc': 'Heisman', '5733ae924776f41900661013': '3,577', '5733cbdad058e614000b6291': 'Shea,', '5733c0064776f41900661198': '88.9', '5733a4c54776f41900660f2d': 'Father', '5733c0064776f4190066119a': '88.9', '5733ccbe4776f41900661271': 'Rudy,', '5733b5344776f419006610de': 'Jenkins,', '57339c184776f41900660ea5': 'England,', '5733cbdad058e614000b628e': 'F.', '5733b5344776f419006610e0': 'Jenkins,', '5733be284776f41900661180': 'Main', '5733b7f74776f41900661130': 'per', '573393e1d058e614000b5dc2': 'Charles', '5733b0fb4776f41900661043': 'Father', '5733caf74776f4190066124c': '1,600', '5733ae924776f41900661017': '750', '573382a14776f41900660c30': '(or', '57339c184776f41900660ea7': 'Engineering', '573393184776f41900660da9': '$9', '573382a14776f41900660c2f': 'Notre', '5733b2fe4776f41900661090': 'Institute.', '57338724d058e614000b5c9f': 'Jesuit', '5733b2fe4776f4190066108f': 'Institute.', '5733ac31d058e614000b5ff4': 'Rev.', '5733b699d058e614000b6119': '29', '5733b699d058e614000b611c': 'years.', '57338724d058e614000b5ca1': 'Father', '5733a55a4776f41900660f3e': 'Hall,', '573383e94776f41900660c5e': 'Prize.', '57339a5bd058e614000b5e95': 'Guglielmino', '573398ebd058e614000b5e69': '83,000', '57338653d058e614000b5c82': 'Around', '5733ad384776f41900660fed': 'Millard', '5733ca05d058e614000b6263': 'games,', '573383494776f41900660c42': 'Division', '5733a3cbd058e614000b5f42': 'Saint', '5733caf74776f4190066124e': '1,600', '5733ad384776f41900660fef': 'Millard', '573394c84776f41900660de1': '1240', '5733adb64776f41900661004': 'million', '5733bf84d058e614000b61be': 'Saint', '5733a3cbd058e614000b5f41': 'Saint', '5733c743d058e614000b6231': 'Army', '5733bed24776f4190066118c': 'center),', '573385394776f41900660c83': 'Father', '57338a51d058e614000b5cf2': 'Father', '5733c1a94776f419006611a9': 'Division', '573388ce4776f41900660cc7': 'Nativism', '573383494776f41900660c45': 'Division', '5733adb64776f41900661003': 'million', '573387acd058e614000b5cb2': 'Division', '573385394776f41900660c7f': 'Father', '5733a4c54776f41900660f2e': 'Father', '5733c29c4776f419006611b8': 'After', '5733a3cbd058e614000b5f43': '33', '5733c0e6d058e614000b61d9': 'workers.', '5733be284776f41900661182': 'Saint', '5733926d4776f41900660d8e': '(now', '5733926d4776f41900660d8f': 'Hall,', '573388ce4776f41900660cc4': 'Nativism', '5733b5df4776f41900661109': '700', '5733b7f74776f4190066112f': 'per', '573387acd058e614000b5cb3': '105', '5733b496d058e614000b60d2': 'Jewish', '573383e94776f41900660c5a': 'Architecture', '573398ebd058e614000b5e68': 'social,', '5733cd504776f41900661291': 'F.', '5733c0064776f41900661199': '88.9', '5733b3d64776f419006610a3': 'Catholic', '5733b2fe4776f41900661093': 'germ-free-life', '573393184776f41900660daa': 'million', '5733a55a4776f41900660f3b': 'Hall,', '5733bf84d058e614000b61bf': 'Saint', '5733b0fb4776f41900661042': 'Father', '5733849bd058e614000b5c59': 'Célestine', '573393184776f41900660da6': '$9', '57338653d058e614000b5c81': 'Main', '573393e1d058e614000b5dc5': 'Charles', '5733b699d058e614000b611a': '29', '57338724d058e614000b5c9d': 'Father', '5733ae924776f41900661015': '750', '573382a14776f41900660c2d': '(or', '5733b3d64776f419006610a5': 'Catholic', '5733c3184776f419006611c3': 'Notre', '5733b1da4776f41900661069': 'Father', '5733ccbe4776f41900661273': 'Rudy,', '5733b2fe4776f41900661091': 'Institute.', '5733a6424776f41900660f4f': 'offered.', '5733ad384776f41900660ff0': 'Sheets.', '5733b1da4776f41900661067': 'Father', '5733c0e6d058e614000b61db': 'workers.', '5733c4494776f419006611da': 'Ohio', '573398ebd058e614000b5e67': 'social,', '573393184776f41900660da8': '$9.7', '573398ebd058e614000b5e6a': 'million.', '5733afd3d058e614000b6047': 'Best', '57339c184776f41900660ea8': '1968.', '573398164776f41900660e23': 'Gregori,', '573393e1d058e614000b5dc3': 'Saint', '5733b5344776f419006610e1': 'Jenkins,', '5733ac31d058e614000b5ff5': 'Theodore', '5733ca05d058e614000b6267': '(e.g.', '573394c84776f41900660de0': '1240', '5733b496d058e614000b60d0': 'Jewish', '57339b36d058e614000b5ea5': 'Silver.', '5733a4c54776f41900660f2f': 'Father', '573393e1d058e614000b5dc6': 'Saint', '5733adb64776f41900661002': 'Hall,', '5733a7bd4776f41900660f6d': 'Engineering', '5733b5df4776f41900661105': '700', '57338a51d058e614000b5cf0': 'Father', '5733c1a94776f419006611aa': 'basketball,', '5733b7f74776f4190066112e': 'Jewish', '573385394776f41900660c82': 'Father', '5733bed24776f41900661188': 'Main', '573383e94776f41900660c5c': '80%', '5733a6424776f41900660f52': 'offered.', '5733b699d058e614000b611b': 'About', '5733b5df4776f41900661106': 'Around', '5733a7bd4776f41900660f6b': '(PhD)', '5733c1a94776f419006611a8': 'Association', '5733a6424776f41900660f51': 'B.S.', '5733ac31d058e614000b5ff3': 'Joan', '5733bed24776f4190066118a': 'Main', '5733a70c4776f41900660f65': 'First', '5733974d4776f41900660e1b': 'Notre', '57339a5bd058e614000b5e94': 'Hall,', '573398164776f41900660e22': 'Luigi', '5733c4494776f419006611dd': 'With', '5733a3cbd058e614000b5f40': 'Saint', '57338a51d058e614000b5cf3': 'Father', '5733a70c4776f41900660f64': 'First', '5733adb64776f41900661001': 'Chemistry', '5733b5344776f419006610df': 'Jenkins,', '57338653d058e614000b5c84': 'Main', '5733c4494776f419006611de': 'Ohio', '5733974d4776f41900660e19': '2005,', '573387acd058e614000b5cb1': 'Division', '5733c29c4776f419006611bc': 'Hockey', '57338a51d058e614000b5cf1': 'Father', '5733c0e6d058e614000b61d8': 'workers.', '5733bed24776f4190066118b': 'Main', '5733cbdad058e614000b628d': 'Shea,', '5733a7bd4776f41900660f6e': 'Engineering', '5733ca05d058e614000b6266': '(e.g.', '5733974d4776f41900660e1a': '2005,', '5733a70c4776f41900660f62': 'First', '573383e94776f41900660c5d': '29', '5733a55a4776f41900660f3a': 'Architecture', '5733b5df4776f41900661107': '700', '5733ae924776f41900661014': '750', '5733c3184776f419006611c2': 'On', '5733a4c54776f41900660f31': 'Father', '573398164776f41900660e24': 'Luigi', '5733bed24776f41900661189': 'Main', '5733c29c4776f419006611b9': 'Hockey', '573394c84776f41900660dde': 'reputation,', '57339b36d058e614000b5ea3': '2012[update]', '5733b2fe4776f41900661092': 'Institute.', '5733c0064776f4190066119b': '88.9', '5733be284776f4190066117e': 'Main', '5733b0fb4776f41900661045': 'Summer', '5733a55a4776f41900660f3d': 'Architecture', '5733afd3d058e614000b6045': 'Best', '5733ca05d058e614000b6264': '(e.g.', '5733c0e6d058e614000b61d7': 'million', '5733b0fb4776f41900661044': 'Summer', '57338653d058e614000b5c85': 'Main', '573383494776f41900660c43': 'Division', '5733c1a94776f419006611a7': 'League', '5733c0e6d058e614000b61da': 'workers.', '5733c29c4776f419006611bb': 'Hockey', '5733ca05d058e614000b6265': 'games,', '573385394776f41900660c81': 'Father', '57339c184776f41900660ea6': '1968.', '5733ae924776f41900661016': '3,577', '5733ad384776f41900660fee': 'Millard', '5733caf74776f4190066124f': '61.'}\n",
      "Epoch: 0, lr: 0.01, Train loss: 10.3576,  Train acc start: 0.0310, Train acc end: 0.0250, Time: 15.7710\n",
      "Evaluation Results: {'exact_match': 1.1152416356877324, 'f1': 8.776184575441079}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4739892e24fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model_on_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparagraphs_mapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestions_mapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcur_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-89d1f67f8889>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, optimizer, loss_function, dataloader, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0manswer_spans_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer_spans_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Run forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mpred_answer_start_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_answer_end_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraph_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Compute the CrossEntropyLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_answer_start_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_spans_start\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_answer_end_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_spans_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/squad_qa_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-c46f5bfed7d0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, paragraphs, questions)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparagraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# As we assume batch_first true, then our sentence_embeddings will have correct shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mparagraphs_seq_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparagraph_embedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraphs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch, seq_len, n_feats * n_dirs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mquestions_seq_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion_embedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch, seq_len, n_feats * n_dirs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# weighted sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/squad_qa_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/squad_qa_pytorch/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    582\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    583\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = {\"train_loss\": [], \"train_acc_start\": [], \"train_acc_end\": []}\n",
    "loop_start = timer()\n",
    "# lr scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5, threshold=0.01)\n",
    "for epoch in range(50):\n",
    "    train_dict = train_step(model, optimizer, loss_function, train_data_loader, device=device)\n",
    "    eval_results = evaluate_model_on_data(model, evaluator, train_data_loader, paragraphs_mapper, questions_mapper, device)\n",
    "    cur_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'Epoch: {epoch}, lr: {cur_lr}, Train loss: {train_dict[\"loss\"]:.4f},  Train acc start: {train_dict[\"accuracy_start\"]:.4f}, Train acc end: {train_dict[\"accuracy_end\"]:.4f}, Time: {train_dict[\"time\"]:.4f}')\n",
    "    history[\"train_loss\"].append(train_dict[\"loss\"]);history[\"train_acc_start\"].append(train_dict[\"accuracy_start\"]);history[\"train_acc_end\"].append(train_dict[\"accuracy_end\"]);\n",
    "    #history[\"val_loss\"].append(val_dict[\"loss\"]);history[\"val_acc\"].append(val_dict[\"accuracy\"]);\n",
    "    #scheduler.step(val_dict[\"loss\"])\n",
    "    print(f\"Evaluation Results: {eval_results}\")\n",
    "loop_end = timer()\n",
    "print(f\"Elapsed time: {(loop_end - loop_start):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "NLP_Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "squad_qa_pytorch",
   "language": "python",
   "name": "squad_qa_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
