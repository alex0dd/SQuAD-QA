{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/alexpod1000/SQuAD-QA/blob/main/Train_model_GPU_DRQA_experimental.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oTkVfFrJ-pzG"
   },
   "outputs": [],
   "source": [
    "# Run the following cells only if using Colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    # Clone repository\n",
    "    !git clone https://github.com/alexpod1000/SQuAD-QA.git\n",
    "    # Change current working directory to match project\n",
    "    %cd SQuAD-QA/\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rsBVuJu6_5qN"
   },
   "outputs": [],
   "source": [
    "# External imports\n",
    "import copy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer, SpaceTokenizer\n",
    "from typing import Tuple, List, Dict, Any, Union\n",
    "\n",
    "# Project imports\n",
    "from squad_data.parser import SquadFileParser\n",
    "from squad_data.utils import build_mappers_and_dataframe, add_paragraphs_spans\n",
    "from evaluation.evaluate import evaluate_predictions\n",
    "from evaluation.utils import build_evaluation_dict\n",
    "from utils import split_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "387a021D9piE"
   },
   "source": [
    "### Download Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xFweP2uIJg8O",
    "outputId": "7bb5b9ca-7b89-4fad-dc90-c979f6ef5f0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-downloaded embeddings from /home/alexpod/uni/magistrale_ai/secondo_anno/nlp/project/SQuAD-QA/embedding_models/embedding_model.kv\n",
      "End!\n",
      "Embedding dimension: 300\n"
     ]
    }
   ],
   "source": [
    "from utils.embedding_utils import EmbeddingDownloader\n",
    "\n",
    "embedding_downloader = EmbeddingDownloader(\n",
    "    \"embedding_models\", \n",
    "    \"embedding_model.kv\", \n",
    "    model_name=\"fasttext-wiki-news-subwords-300\"\n",
    ")\n",
    "\n",
    "embedding_model = embedding_downloader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Rh4dSW-9tYm"
   },
   "source": [
    "### Parse the json and get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FAEEYoypAOKA"
   },
   "outputs": [],
   "source": [
    "train_file_json = \"squad_data/data/training_set.json\"\n",
    "test_file_json = \"squad_data/data/dev-v1.1.json\"\n",
    "\n",
    "train_parser = SquadFileParser(train_file_json)\n",
    "test_parser = SquadFileParser(test_file_json)\n",
    "\n",
    "train_data = train_parser.parse_documents()\n",
    "test_data = test_parser.parse_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKK-4d1_93QE"
   },
   "source": [
    "### Prepare the mappers and datafram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "x-y1GLEZJPvA",
    "outputId": "20f8a400-7f2a-406f-cd8d-87dd5698559d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>question_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0_0</td>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>0</td>\n",
       "      <td>515</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0_0</td>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0_0</td>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>0</td>\n",
       "      <td>279</td>\n",
       "      <td>the Main Building</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0_0</td>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>0</td>\n",
       "      <td>381</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0_0</td>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id paragraph_id               question_id  answer_id  answer_start  \\\n",
       "0       0          0_0  5733be284776f41900661182          0           515   \n",
       "1       0          0_0  5733be284776f4190066117f          0           188   \n",
       "2       0          0_0  5733be284776f41900661180          0           279   \n",
       "3       0          0_0  5733be284776f41900661181          0           381   \n",
       "4       0          0_0  5733be284776f4190066117e          0            92   \n",
       "\n",
       "                               answer_text  \\\n",
       "0               Saint Bernadette Soubirous   \n",
       "1                a copper statue of Christ   \n",
       "2                        the Main Building   \n",
       "3  a Marian place of prayer and reflection   \n",
       "4       a golden statue of the Virgin Mary   \n",
       "\n",
       "                                       question_text  \n",
       "0  To whom did the Virgin Mary allegedly appear i...  \n",
       "1  What is in front of the Notre Dame Main Building?  \n",
       "2  The Basilica of the Sacred heart at Notre Dame...  \n",
       "3                  What is the Grotto at Notre Dame?  \n",
       "4  What sits on top of the Main Building at Notre...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs_mapper, df = build_mappers_and_dataframe(train_data, limit_answers=1)\n",
    "print(paragraphs_mapper[next(iter(paragraphs_mapper))])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text_dict: Dict[str, Any], text_key: Union[str, None] = None) -> Any:\n",
    "    text_dict = copy.deepcopy(text_dict)\n",
    "    # just tokenize and remove punctuation for now\n",
    "    tokenizer = SpaceTokenizer()#TreebankWordTokenizer()\n",
    "    for key in text_dict.keys():\n",
    "        if text_key is not None:\n",
    "            text = tokenizer.tokenize(text_dict[key][text_key])\n",
    "            text_dict[key][text_key] = text\n",
    "        else:\n",
    "            text = tokenizer.tokenize(text_dict[key])\n",
    "            text_dict[key] = text\n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_mapper = preprocess_text(paragraphs_mapper)\n",
    "df['question_text'] = df.apply(lambda row: nltk.word_tokenize(row['question_text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5hpoaGpK4qYe"
   },
   "outputs": [],
   "source": [
    "# Extend the paragraphs mapper to include spans\n",
    "paragraphs_spans_mapper = add_paragraphs_spans(paragraphs_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E02_XWu_4qYe",
    "outputId": "9d318e81-3fac-4d9c-b214-865708f5c71d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Architecturally,', 'the', 'school', 'has', 'a', 'Catholic', 'character.', 'Atop', 'the', 'Main', \"Building's\", 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'Virgin', 'Mary.', 'Immediately', 'in', 'front', 'of', 'the', 'Main', 'Building', 'and', 'facing', 'it,', 'is', 'a', 'copper', 'statue', 'of', 'Christ', 'with', 'arms', 'upraised', 'with', 'the', 'legend', '\"Venite', 'Ad', 'Me', 'Omnes\".', 'Next', 'to', 'the', 'Main', 'Building', 'is', 'the', 'Basilica', 'of', 'the', 'Sacred', 'Heart.', 'Immediately', 'behind', 'the', 'basilica', 'is', 'the', 'Grotto,', 'a', 'Marian', 'place', 'of', 'prayer', 'and', 'reflection.', 'It', 'is', 'a', 'replica', 'of', 'the', 'grotto', 'at', 'Lourdes,', 'France', 'where', 'the', 'Virgin', 'Mary', 'reputedly', 'appeared', 'to', 'Saint', 'Bernadette', 'Soubirous', 'in', '1858.', 'At', 'the', 'end', 'of', 'the', 'main', 'drive', '(and', 'in', 'a', 'direct', 'line', 'that', 'connects', 'through', '3', 'statues', 'and', 'the', 'Gold', 'Dome),', 'is', 'a', 'simple,', 'modern', 'stone', 'statue', 'of', 'Mary.']\n",
      "[(0, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 53), (54, 58), (59, 62), (63, 67), (68, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 232), (233, 237), (238, 241), (242, 248), (249, 256), (257, 259), (260, 262), (263, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 333), (334, 345), (346, 352), (353, 356), (357, 365), (366, 368), (369, 372), (373, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 451), (452, 454), (455, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 502), (503, 511), (512, 514), (515, 520), (521, 531), (532, 541), (542, 544), (545, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 653), (654, 656), (657, 658), (659, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 695)]\n"
     ]
    }
   ],
   "source": [
    "print(paragraphs_spans_mapper['0_0']['text'])\n",
    "print(paragraphs_spans_mapper['0_0']['spans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = split_dataframe(df, train_ratio=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 87599, Train samples: 60876, Validation samples: 26723\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total samples: {len(df)}, Train samples: {len(df_train)}, Validation samples: {len(df_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xb4YK_Qa95zK"
   },
   "source": [
    "### DataConverter and CustomQADataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KDl4CIW-mj_D",
    "outputId": "f012b0d4-f66e-4917-c208-1ebebe9a2f23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 162, 300])\n",
      "torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "from data_loading.utils import DataConverter, padder_collate_fn\n",
    "from data_loading.qa_dataset import CustomQADataset\n",
    "\n",
    "data_converter = DataConverter(embedding_model, paragraphs_spans_mapper)\n",
    "datasetQA = CustomQADataset(data_converter, df_train, paragraphs_mapper)\n",
    "data_loader = torch.utils.data.DataLoader(datasetQA, collate_fn = padder_collate_fn, batch_size=10, shuffle=True)\n",
    "\n",
    "test_batch = next(iter(data_loader))\n",
    "print(test_batch[\"paragraph_emb\"].shape)\n",
    "print(test_batch[\"y_gt\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.utils import SpanExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"The device is {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model:\n",
    "\n",
    "(paragraph_emb, question_emb) -> (answer_start, answer_end) // for each token in paragraph_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, loss_function, dataloader, device=\"cpu\", show_progress=False):\n",
    "    acc_loss = 0\n",
    "    acc_start_accuracy = 0\n",
    "    acc_end_accuracy = 0\n",
    "    count = 0\n",
    "\n",
    "    time_start = timer()\n",
    "    \n",
    "    model.train()\n",
    "    wrapped_dataloader = tqdm(dataloader) if show_progress else dataloader\n",
    "    for batch in wrapped_dataloader:\n",
    "        answer_spans_start = batch[\"y_gt\"][:, 0]\n",
    "        answer_spans_end = batch[\"y_gt\"][:, 1]\n",
    "        # Clear gradients\n",
    "        model.zero_grad()\n",
    "        # Place to right device\n",
    "        answer_spans_start = answer_spans_start.to(device)\n",
    "        answer_spans_end = answer_spans_end.to(device)\n",
    "        # Run forward pass\n",
    "        pred_answer_start_scores, pred_answer_end_scores = model(batch)\n",
    "        # Compute the CrossEntropyLoss\n",
    "        loss = loss_function(pred_answer_start_scores, answer_spans_start) + loss_function(pred_answer_end_scores, answer_spans_end)\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        # --- Compute metrics ---\n",
    "        # Get span indexes\n",
    "        pred_span_start_idxs, pred_span_end_idxs = SpanExtractor.extract_most_probable(pred_answer_start_scores, pred_answer_end_scores)\n",
    "        gt_start_idxs = answer_spans_start.cpu().detach()\n",
    "        gt_end_idxs = answer_spans_end.cpu().detach()\n",
    "        # two accs\n",
    "        start_accuracy = torch.sum(gt_start_idxs == pred_span_start_idxs) / len(pred_span_start_idxs)\n",
    "        end_accuracy = torch.sum(gt_end_idxs == pred_span_end_idxs) / len(pred_span_end_idxs)\n",
    "        # Gather stats\n",
    "        acc_loss += loss.item()\n",
    "        acc_start_accuracy += start_accuracy.item()\n",
    "        acc_end_accuracy += end_accuracy.item()\n",
    "        count += 1\n",
    "    time_end = timer()\n",
    "    return {\n",
    "        \"loss\": acc_loss / count, \n",
    "        \"accuracy_start\": acc_start_accuracy / count, \n",
    "        \"accuracy_end\": acc_end_accuracy / count,\n",
    "        \"time\": time_end - time_start\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(model, loss_function, dataloader, device=\"cpu\", show_progress=False):\n",
    "    acc_loss = 0\n",
    "    acc_start_accuracy = 0\n",
    "    acc_end_accuracy = 0\n",
    "    count = 0\n",
    "\n",
    "    time_start = timer()\n",
    "    wrapped_dataloader = tqdm(dataloader) if show_progress else dataloader\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in wrapped_dataloader:\n",
    "            answer_spans_start = batch[\"y_gt\"][:, 0]\n",
    "            answer_spans_end = batch[\"y_gt\"][:, 1]\n",
    "            # Place to right device\n",
    "            answer_spans_start = answer_spans_start.to(device)\n",
    "            answer_spans_end = answer_spans_end.to(device)\n",
    "            # Run forward pass\n",
    "            pred_answer_start_scores, pred_answer_end_scores = model(batch)\n",
    "            # Compute the CrossEntropyLoss\n",
    "            loss = loss_function(pred_answer_start_scores, answer_spans_start) + loss_function(pred_answer_end_scores, answer_spans_end)\n",
    "            # --- Compute metrics ---\n",
    "            # Get span indexes\n",
    "            pred_span_start_idxs, pred_span_end_idxs = SpanExtractor.extract_most_probable(pred_answer_start_scores, pred_answer_end_scores)\n",
    "            gt_start_idxs = answer_spans_start.cpu().detach()\n",
    "            gt_end_idxs = answer_spans_end.cpu().detach()\n",
    "            # two accs\n",
    "            start_accuracy = torch.sum(gt_start_idxs == pred_span_start_idxs) / len(pred_span_start_idxs)\n",
    "            end_accuracy = torch.sum(gt_end_idxs == pred_span_end_idxs) / len(pred_span_end_idxs)\n",
    "            # Gather stats\n",
    "            acc_loss += loss.item()\n",
    "            acc_start_accuracy += start_accuracy.item()\n",
    "            acc_end_accuracy += end_accuracy.item()\n",
    "            count += 1\n",
    "    time_end = timer()\n",
    "    return {\n",
    "        \"loss\": acc_loss / count, \n",
    "        \"accuracy_start\": acc_start_accuracy / count, \n",
    "        \"accuracy_end\": acc_end_accuracy / count,\n",
    "        \"time\": time_end - time_start\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSum(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"\n",
    "        General idea, given a random dummy weights vector, \n",
    "        learn to weight it based on query\n",
    "        \"\"\"\n",
    "        super(WeightedSum, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(input_dim))\n",
    "\n",
    "    def forward(self, input_emb, mask=None):\n",
    "        # TODO: if needed, implement time masking\n",
    "        batch, timesteps, embed_dim = input_emb.shape\n",
    "        # w dot q_j\n",
    "        dot_prods = torch.matmul(input_emb, self.weights)\n",
    "        # exp(w dot q_j)\n",
    "        exp_prods = torch.exp(dot_prods)\n",
    "        # normalization factor\n",
    "        sum_exp_prods = torch.sum(exp_prods, dim=1)\n",
    "        sum_exp_prods = sum_exp_prods.repeat(timesteps, 1).T\n",
    "        # b_j\n",
    "        b = exp_prods / sum_exp_prods\n",
    "        # q (embedding) = sum_t(b_t * q_t)\n",
    "        b_scal_q = input_emb * b[:, :, None]\n",
    "        # now sum along correct axis\n",
    "        q = torch.sum(b_scal_q, axis=1)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compatibility functions**\n",
    "\n",
    "**Multiplicative (dot)**:\n",
    "\n",
    "p = paragraph emb shape: [B, T, E] (Query)\n",
    "\n",
    "q = question weighted shape: [B, E] reshaped to [B, E, 1] (Keys)\n",
    "\n",
    "scores = p @ q (of shape: [B, T, 1])\n",
    "\n",
    "**General bilinear**:\n",
    "\n",
    "p = paragraph emb shape: [B, T, Ep] (Query)\n",
    "\n",
    "q = question weighted shape: [B, Eq] reshaped to [B, Eq, 1] (Keys)\n",
    "\n",
    "W = parameter matrix of shape: [Ep, Eq]\n",
    "\n",
    "scores = p @ W @ q (of shape: [B, T, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearCompatibility(nn.Module):\n",
    "    def __init__(self, query_dim, keys_dim):\n",
    "        \"\"\"\n",
    "        Perform bilinear compatibility f(q, K) = q.T @ W @ K\n",
    "        Recall: multiplicative/dot compatibility is f(q, K) = q.T @ K\n",
    "        \n",
    "        Where: \n",
    "            q -> embedded paragraphs (p in DrQA)\n",
    "            K -> embedded question (q in DrQA)\n",
    "        \"\"\"\n",
    "        super(BilinearCompatibility, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(query_dim, keys_dim))\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        \"\"\"\n",
    "        query: batch of shape (batch, seq_len, query_dim) (Query)\n",
    "        keys = batch of shape (batch, key_dim) which will be reshaped into [batch, key_dim, 1] (Keys)\n",
    "        \"\"\"\n",
    "        return query @ self.weights @ keys[:, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_QA(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(LSTM_QA, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.paragraph_embedder = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.question_embedder = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.weighted_sum = WeightedSum(hidden_dim * 2)\n",
    "        # used to compute similarity scores\n",
    "        self.general_bilinear_start = BilinearCompatibility(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.general_bilinear_end = BilinearCompatibility(hidden_dim * 2, hidden_dim * 2)\n",
    "        # to classify from similarity to prob of start and prob of end\n",
    "        self.sim_to_start = nn.Linear(1, 1) # given a similarity score, predict P(start)\n",
    "        self.sim_to_end = nn.Linear(1, 1) # given a similarity score, predict P(end)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Extract data from inputs dictionary and put it on right device\n",
    "        curr_device = next(self.parameters()).device # trick to get current device from the params\n",
    "        paragraphs = inputs[\"paragraph_emb\"].to(curr_device)\n",
    "        questions = inputs[\"question_emb\"].to(curr_device)\n",
    "        # Perform the normal forward pass\n",
    "        batch_size, seq_len, n_feat = paragraphs.shape\n",
    "        # As we assume batch_first true, then our sentence_embeddings will have correct shape\n",
    "        paragraphs_seq_emb, _ = self.paragraph_embedder(paragraphs) # (batch, seq_len, n_feats * n_dirs)\n",
    "        questions_seq_emb, _ = self.question_embedder(questions) # (batch, seq_len, n_feats * n_dirs)\n",
    "        # weighted sum\n",
    "        questions_state_repr = self.weighted_sum(questions_seq_emb)\n",
    "        #return paragraphs_seq_emb, questions_state_repr\n",
    "        # compute similarities -> (batch, timestep, 1)\n",
    "        similarities_start = self.general_bilinear_start(paragraphs_seq_emb, questions_state_repr)\n",
    "        similarities_end = self.general_bilinear_start(paragraphs_seq_emb, questions_state_repr)\n",
    "        # --- Given a similarity score, predict P(start), P(end) ---\n",
    "        # similarities flattened\n",
    "        similarities_start = similarities_start.contiguous()\n",
    "        similarities_start = similarities_start.view(-1, 1) # as similarity dim is 1 -> viewed shape is (batch*timestep, 1)\n",
    "        start_scores = self.sim_to_start(similarities_start)\n",
    "        start_logits = start_scores.view(batch_size, seq_len) # P(start)\n",
    "        \n",
    "        similarities_end = similarities_end.contiguous()\n",
    "        similarities_end = similarities_end.view(-1, 1) # as similarity dim is 1 -> viewed shape is (batch*timestep, 1)\n",
    "        end_scores = self.sim_to_end(similarities_end)\n",
    "        end_logits = end_scores.view(batch_size, seq_len) # P(end)\n",
    "        \n",
    "        # if we view each sequence of tokens as a feature vector\n",
    "        # we can interpret the start/end assignation problem as \n",
    "        # a classification with a variable number of classes\n",
    "        # thus assume that our model outputs logits that will just be passed\n",
    "        # to a softmax, to build a probable distribution of the start token\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline model\n",
    "model = LSTM_QA(embedding_model.vector_size, 128).to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_converter = DataConverter(embedding_model, paragraphs_spans_mapper)\n",
    "dataset_train_QA = CustomQADataset(data_converter, df_train, paragraphs_mapper)\n",
    "dataset_val_QA = CustomQADataset(data_converter, df_val, paragraphs_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(dataset_train_QA, collate_fn = padder_collate_fn, batch_size=128, shuffle=True)\n",
    "val_data_loader = torch.utils.data.DataLoader(dataset_val_QA, collate_fn = padder_collate_fn, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    \"train_loss\": [], \"train_acc_start\": [], \"train_acc_end\": [],\n",
    "    \"val_loss\": [], \"val_acc_start\": [], \"val_acc_end\": []\n",
    "}\n",
    "loop_start = timer()\n",
    "# lr scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5, threshold=0.01)\n",
    "for epoch in range(50):\n",
    "    train_dict = train_step(model, optimizer, loss_function, train_data_loader, device=device, show_progress=True)\n",
    "    val_dict = validation_step(model, loss_function, val_data_loader, device=device, show_progress=True)\n",
    "    cur_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'Epoch: {epoch}, '\n",
    "          f'lr: {cur_lr}, '\n",
    "          f'Train loss: {train_dict[\"loss\"]:.4f}, '\n",
    "          f'Train acc start: {train_dict[\"accuracy_start\"]:.4f}, '\n",
    "          f'Train acc end: {train_dict[\"accuracy_end\"]:.4f}, '\n",
    "          f'Val loss: {val_dict[\"loss\"]:.4f}, '\n",
    "          f'Val acc start: {val_dict[\"accuracy_start\"]:.4f}, '\n",
    "          f'Val acc end: {val_dict[\"accuracy_end\"]:.4f}, '\n",
    "          f'Time: {train_dict[\"time\"]:.4f}')\n",
    "    history[\"train_loss\"].append(train_dict[\"loss\"]);history[\"train_acc_start\"].append(train_dict[\"accuracy_start\"]);history[\"train_acc_end\"].append(train_dict[\"accuracy_end\"]);\n",
    "    history[\"val_loss\"].append(val_dict[\"loss\"]);history[\"val_acc_start\"].append(val_dict[\"accuracy_start\"]);history[\"val_acc_end\"].append(val_dict[\"accuracy_end\"]);\n",
    "    #scheduler.step(val_dict[\"loss\"])\n",
    "loop_end = timer()\n",
    "print(f\"Elapsed time: {(loop_end - loop_start):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paragraphs_mapper, df_test = build_mappers_and_dataframe(test_data, limit_answers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paragraphs_mapper = preprocess_text(test_paragraphs_mapper)\n",
    "df_test['question_text'] = df_test.apply(lambda row: nltk.word_tokenize(row['question_text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend the paragraphs mapper to include spans\n",
    "test_paragraphs_spans_mapper = add_paragraphs_spans(test_paragraphs_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_converter.paragraphs_spans_mapper = test_paragraphs_spans_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_QA = CustomQADataset(data_converter, df_test, test_paragraphs_mapper)\n",
    "test_data_loader = torch.utils.data.DataLoader(dataset_test_QA, collate_fn = padder_collate_fn, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_file_json, \"r\") as f:\n",
    "    dataset_json = json.load(f)\n",
    "pred_dict = build_evaluation_dict(model, test_data_loader, test_paragraphs_mapper, tokenizer, device, show_progress=True)\n",
    "eval_results = evaluate_predictions(dataset_json, pred_dict)\n",
    "print(eval_results)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "NLP_Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
